{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of RLAlgo-6-DDPG-Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarsanjani/meansquares/blob/master/temp/Copy_of_RLAlgo_6_DDPG_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prYhQX_dTzVC"
      },
      "source": [
        "> This notebook for Deep Deterministic Policy Gradient (DDPG) belongs to an intallment of notebooks that address the technical implementation of modern RL algorithms. See [this notebook](https://colab.research.google.com/drive/1cDq73ac6N67IPGBqonCH2dyjrmAkclkM) for a description of the DDPG algorithm.\n",
        "\n",
        "The main references for this notebook are: \n",
        "\n",
        "[1]  [Deep Deterministic Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#deep-deterministic-policy-gradient)\n",
        "\n",
        "[2] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oow0rc2iaDZ4"
      },
      "source": [
        "# CoLab Preambles\n",
        "\n",
        "Most of the requirements of python packages are already fulfilled on CoLab. To run Gym, you have to install prerequisites like xvbf,opengl & other python-dev packages using the following codes.\n",
        "\n",
        "[](To be done next time: )\n",
        "[](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acRAoifbwiMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7faeb47-e72f-45d9-a313-78832a2dd3e7"
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "\n",
        "# Special gym environment\n",
        "!pip install gym[atari]\n",
        "\n",
        "# For rendering environment, you can use pyvirtualdisplay.\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "\n",
        "# Install spinningup on CoLab\n",
        "!git clone https://github.com/openai/spinningup.git\n",
        "!cd spinningup\n",
        "#!pip install -e . # this will incur error: File \"setup.py\" not found. Directory cannot be installed in editable mode: /content\n",
        "!pip install -e spinningup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.7)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.0.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.35.1)\n",
            "fatal: destination path 'spinningup' already exists and is not an empty directory.\n",
            "Obtaining file:///content/spinningup\n",
            "Requirement already satisfied: cloudpickle==1.2.1 in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (1.2.1)\n",
            "Requirement already satisfied: gym[atari,box2d,classic_control]~=0.15.3 in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (0.15.7)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (5.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (0.17.0)\n",
            "Requirement already satisfied: matplotlib==3.1.1 in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (3.1.1)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (1.1.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (3.6.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (5.4.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (1.4.1)\n",
            "Requirement already satisfied: seaborn==0.8.1 in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (0.8.1)\n",
            "Requirement already satisfied: tensorflow<2.0,>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (1.15.4)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from spinup==0.2.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup==0.2.0) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup==0.2.0) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup==0.2.0) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup==0.2.0) (7.0.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup==0.2.0) (2.3.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->spinup==0.2.0) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->spinup==0.2.0) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->spinup==0.2.0) (50.3.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->spinup==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->spinup==0.2.0) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->spinup==0.2.0) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->spinup==0.2.0) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->spinup==0.2.0) (4.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1->spinup==0.2.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1->spinup==0.2.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1->spinup==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1->spinup==0.2.0) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->spinup==0.2.0) (2018.9)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spinup==0.2.0) (8.6.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spinup==0.2.0) (20.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spinup==0.2.0) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spinup==0.2.0) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->spinup==0.2.0) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (1.33.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (0.35.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (1.15.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.8.0->spinup==0.2.0) (0.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,box2d,classic_control]~=0.15.3->spinup==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->spinup==0.2.0) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->spinup==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->spinup==0.2.0) (0.6.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.8.0->spinup==0.2.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.8.0->spinup==0.2.0) (3.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<2.0,>=1.8.0->spinup==0.2.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.8.0->spinup==0.2.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.8.0->spinup==0.2.0) (3.4.0)\n",
            "Installing collected packages: spinup\n",
            "  Found existing installation: spinup 0.2.0\n",
            "    Can't uninstall 'spinup'. No files were found to uninstall.\n",
            "  Running setup.py develop for spinup\n",
            "Successfully installed spinup\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIWCOR4hukvR"
      },
      "source": [
        "# To activate virtual display \n",
        "# need to run a script once for training an agent as follows\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1\n",
        "\n",
        "#\n",
        "# Import libraries\n",
        "#\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWCulc7UWLMI"
      },
      "source": [
        "# Deep Deterministic Policy Gradient\n",
        "\n",
        "[DDPG@SpinningUp](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)\n",
        "\n",
        "### Pseudocode\n",
        "\n",
        "![DDPG-Pseudocode](https://spinningup.openai.com/en/latest/_images/math/66446594f760581068c5684f053e75f3de9b1404.svg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBzq6YrmhHTA"
      },
      "source": [
        "[](https://github.com/gabrielgarza/openai-gym-policy-gradient)\n",
        "[](https://github.com/yukezhu/tensorflow-reinforce/tree/master/rl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyTcBk-biyvJ"
      },
      "source": [
        "### Define MLP Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vYT9NoIiyXc"
      },
      "source": [
        "# mlp.py\n",
        "# define MLP\n",
        "\n",
        "def mlp(x, hidden_sizes=(32,), \n",
        "           activation=tf.tanh,\n",
        "           output_activation=None):\n",
        "  \"\"\"\n",
        "    Multi-Layer Perceptron (MLP) FC\n",
        "    Construct a feedforward neural network\n",
        "    INPUT: input x\n",
        "    OUTPUT: MLP computation graphs\n",
        "  \"\"\"\n",
        "  for hs in hidden_sizes[:-1]:\n",
        "    x = tf.layers.dense(x, units=hs, activation=activation)\n",
        "  return tf.layers.dense(x, units=hidden_sizes[-1], activation=output_activation)\n",
        "\n",
        "def mlp_actor_critic(x, a,\n",
        "           hidden_sizes=(256, 256), \n",
        "           activation=tf.nn.relu,\n",
        "           output_activation=tf.tanh,\n",
        "           action_space=None):\n",
        "  \"\"\"\n",
        "    Actor-Critics for DDPG\n",
        "    constructs 4 neural networks\n",
        "    1. running Q-function approx\n",
        "    2. target Q-function approx\n",
        "    3. running policy approx\n",
        "    4. target policy approx\n",
        "  \"\"\"\n",
        "  act_dim = a.shape.as_list()[-1]\n",
        "  print(\"before with, act_dim= \",act_dim)\n",
        "  act_limit = action_space.high[0]\n",
        "  print(\"act_limit \", act_limit)\n",
        "  with tf.variable_scope('pi'):\n",
        "    print(\"calling pi in actor_critic\")\n",
        "    print(\"hidden_size= \",hidden_sizes)\n",
        "    print(\"act_dim= \",act_dim)\n",
        "    pi = act_limit * mlp(x, list(hidden_sizes)+[act_dim], activation, output_activation)\n",
        "  with tf.variable_scope('q'):\n",
        "    print(\"calling q in actor_critic\")\n",
        "    print(\"hidden_size= \",hidden_sizes)\n",
        "    q = tf.squeeze(mlp(tf.concat([x,a], axis=-1), list(hidden_sizes)+[1], activation, None), axis=1)\n",
        "  with tf.variable_scope('q', reuse=True):\n",
        "    print(\"calling q 2nd time in actor_critic\")\n",
        "    print(\"hidden_size= \",hidden_sizes)\n",
        "    q_pi = tf.squeeze(mlp(tf.concat([x,pi], axis=-1), list(hidden_sizes)+[1], activation, None), axis=1)\n",
        "  return pi, q, q_pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGR9B4DQhsR_"
      },
      "source": [
        "### Define DDPG\n",
        "\n",
        "- INPUT\n",
        "\n",
        "- OUTPUT\n",
        "\n",
        "[](https://spinningup.openai.com/en/latest/_modules/spinup/algos/ddpg/ddpg.html#ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3s-ewteGDCj"
      },
      "source": [
        "# utils.py\n",
        "# utils functions\n",
        "\n",
        "def get_vars(scope):\n",
        "    return [x for x in tf.global_variables() if scope in x.name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSJ6itUgvmaX"
      },
      "source": [
        "# ddpg.py\n",
        "# reference: openai spinup ddpy implementation\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# use spinup EpochLogger \n",
        "from spinup.utils.logx import EpochLogger\n",
        "\n",
        "# self defined functions\n",
        "# or just run the previous \n",
        "# mlp.py\n",
        "# utils.py\n",
        "# from mlp import mlp_actor_critic\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "  \"\"\"\n",
        "  Experience replay buffer for DDPG agents: \n",
        "  - countinous obs space\n",
        "  - countinous act space\n",
        "  \"\"\"\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "    self.obs1_buff = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.acts_buff = np.zeros([size, act_dim], dtype=np.float32)\n",
        "    self.obs2_buff = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.rews_buff = np.zeros(size, dtype=np.float32)\n",
        "    self.done_buff = np.zeros(size, dtype=np.float32)\n",
        "    self.cur_ptr, self.cur_size, self.max_size = 0, 0, size\n",
        "    \n",
        "  def store(self, obs, act, rew, next_obs, done):\n",
        "    print(\"--- storing to ReplayBuffer --- \")\n",
        "    self.obs1_buff[self.cur_ptr] = obs\n",
        "    self.acts_buff[self.cur_ptr] = act\n",
        "    self.obs2_buff[self.cur_ptr] = next_obs\n",
        "    self.rews_buff[self.cur_ptr] = rew\n",
        "    self.done_buff[self.cur_ptr] = done\n",
        "    self.cur_ptr = (self.cur_ptr + 1) % self.max_size\n",
        "    self.cur_size = min(self.cur_size + 1, self.max_size)\n",
        "    \n",
        "  def sample_batch(self, batch_size=32):\n",
        "    idxs = np.random.randint(0, self.cur_size, size=batch_size)\n",
        "    samples = dict(obs1 = self.obs1_buff[idxs],\n",
        "                   acts = self.acts_buff[idxs],\n",
        "                   obs2 = self.obs2_buff[idxs],\n",
        "                   rews = self.rews_buff[idxs],\n",
        "                   done = self.done_buff[idxs])\n",
        "    return(samples)\n",
        "  \n",
        "\"\"\"\n",
        "Deep Deterministic Policy Gradient (DDPG)\n",
        "\"\"\"  \n",
        "def ddpg(env_fn, \n",
        "         actor_critic = mlp_actor_critic,\n",
        "         ac_kwargs = dict(),\n",
        "         seed = 0, \n",
        "         steps_per_epoch = 5000,\n",
        "         epochs = 100,\n",
        "         replay_size = int(1e6),\n",
        "         gamma = 0.99,\n",
        "         polyak = 0.995,\n",
        "         pi_lr = 1e-3,\n",
        "         q_lr = 1e-3,\n",
        "         batch_size = 100,\n",
        "         start_steps = 10000,\n",
        "         act_noise = 0.1,\n",
        "         max_ep_len = 1000,\n",
        "         logger_kwargs = dict(),\n",
        "         save_freq = 1):\n",
        "  \"\"\"\n",
        "  DDPG\n",
        "  \"\"\"\n",
        "  \n",
        "  # use spinup util logger\n",
        "  if not logger_kwargs == None:\n",
        "    logger = EpochLogger(**logger_kwargs)\n",
        "    logger.save_config(locals())\n",
        "  \n",
        "  # random seed\n",
        "  tf.set_random_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  \n",
        "  env, test_env = env_fn(), env_fn()\n",
        "  obs_dim = env.observation_space.shape[0] # continuous\n",
        "  act_dim = env.action_space.shape[0] # does not work for discrete() space\n",
        "  print(\"ddpg obs_dim \", obs_dim)\n",
        "  print(\"ddpg act_dim \", act_dim)\n",
        "\n",
        "  # action limit: assume all dimensions share the same bound\n",
        "  act_limit = env.action_space.high[0]\n",
        "  print(\"ddpg act_limit \", act_limit)\n",
        "\n",
        "  # share information about action space with policy architecture\n",
        "  ac_kwargs['action_space'] = env.action_space\n",
        "  print(\"printing env.action_space \", env.action_space)\n",
        "\n",
        "  # inputs to the computation graph\n",
        "  x_ph = tf.placeholder(dtype=tf.float32, shape=(None, obs_dim))\n",
        "  a_ph = tf.placeholder(dtype=tf.float32, shape=(None, act_dim))\n",
        "  x2_ph = tf.placeholder(dtype=tf.float32, shape=(None, obs_dim))\n",
        "  r_ph = tf.placeholder(dtype=tf.float32, shape=(None, ))\n",
        "  d_ph = tf.placeholder(dtype=tf.float32, shape=(None, ))\n",
        "  print(\"****** X_ph ****\")\n",
        "  print(x_ph)\n",
        "  print(\"****** a_ph ****\")\n",
        "  print(a_ph)\n",
        "  \n",
        "  # outputs from the computation graph\n",
        "  # curr policy and Q-function\n",
        "  with tf.variable_scope('running'):\n",
        "    \n",
        "    print(\"**********running**********\")\n",
        "    pi, q, q_pi = actor_critic(x_ph, a_ph, **ac_kwargs)\n",
        "  \n",
        "  # target policy and Q-function\n",
        "  with tf.variable_scope('target'):\n",
        "    print('******target*********')\n",
        "    # Note that the action placeholder a_ph here is irrelevant\n",
        "    # because we only need q_targ(s, pi_targ(s))\n",
        "    pi_targ, _, q_pi_targ = actor_critic(x2_ph, a_ph, **ac_kwargs)\n",
        "  \n",
        "  # experience buffer \n",
        "  replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
        "  \n",
        "  # count variables and print information\n",
        "  # To be done ...\n",
        "  \n",
        "  #################################################\n",
        "  ##\n",
        "  ## Define components of the computation graph\n",
        "  ##\n",
        "  \n",
        "  # Bellman update for Q function\n",
        "  # do not take graident in this part\n",
        "  # use target q and pi to stablize computation\n",
        "  q_bell = tf.stop_gradient(r_ph + gamma*(1-d_ph)*q_pi_targ)\n",
        "  \n",
        "  # DDPG losses for policy and q function, respectively\n",
        "  pi_loss = -tf.reduce_mean(q_pi)\n",
        "  q_loss = tf.reduce_mean((q - q_bell)**2)\n",
        "  print(\"-- updated loss and q values -- \")\n",
        "  # separately train ops for pi, q\n",
        "  pi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)\n",
        "  q_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)\n",
        "  train_pi_opt = pi_optimizer.minimize(pi_loss, var_list=get_vars('running/pi'))\n",
        "  train_q_opt = q_optimizer.minimize(q_loss, var_list=get_vars('running/q'))\n",
        "  print(\"-- updated loss and q values with optimizer -- \")\n",
        "  # polyak average update target pi and q function variables\n",
        "  target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_running) \n",
        "                            for v_running, v_targ in zip(get_vars('running'), get_vars('target'))])\n",
        "  print(\"polyak average update target pi and q function variables\")\n",
        "  # initialize targets to be the same as the running variable\n",
        "  target_init = tf.group([tf.assign(v_targ, v_running)\n",
        "                          for v_running, v_targ in zip(get_vars('running'), get_vars('target'))])\n",
        "  print(\"initialize targets to be the same as the running variable\")\n",
        "  #################################################\n",
        "  ##\n",
        "  ## Run session to train the computation graph\n",
        "  ##\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(target_init)\n",
        "  \n",
        "  # setup model saving \n",
        "  logger.setup_tf_saver(sess, inputs={'x': x_ph, 'a': a_ph}, outputs={'pi': pi, 'q': q})\n",
        "  \n",
        "  # action exploration or exploitation\n",
        "  # later: eps-greedy or UCB\n",
        "  def get_action(obs, noise_scale):\n",
        "    a = sess.run(pi, feed_dict={x_ph: obs.reshape(1,-1)})[0]\n",
        "    a += noise_scale * np.random.randn(act_dim)\n",
        "    return np.clip(a, -act_limit, act_limit)\n",
        "  \n",
        "  start_time = time.time()\n",
        "  o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "  total_steps = steps_per_epoch * epochs\n",
        "  print(\"start_steps : \",start_steps)\n",
        "  # Main loop: collect experience in env and update/log each epoch\n",
        "  for t in range(total_steps):\n",
        "    \n",
        "    # Get action\n",
        "    if t > start_steps:\n",
        "      a = get_action(o, act_noise)\n",
        "    else:\n",
        "      a = env.action_space.sample()\n",
        "\n",
        "    # Step the env\n",
        "    o2, r, d, _ = env.step(a)\n",
        "    ep_ret += r\n",
        "    ep_len += 1\n",
        "    d = False if ep_len==max_ep_len else d\n",
        "    replay_buffer.store(o, a, r, o2, d)\n",
        "    o = o2\n",
        "      \n",
        "    # update DDPG at the end of the trajectory\n",
        "    if d or (ep_len == max_ep_len):\n",
        "      for _ in range(ep_len):\n",
        "        batch = replay_buffer.sample_batch(batch_size)\n",
        "        feed_dict = {x_ph: batch['obs1'],\n",
        "                     x2_ph: batch['obs2'],\n",
        "                     a_ph: batch['acts'],\n",
        "                     r_ph: batch['rews'],\n",
        "                     d_ph: batch['done']\n",
        "                    }\n",
        "          \n",
        "        # Q-learning update\n",
        "        outs = sess.run([q_loss, q, train_q_opt], feed_dict)\n",
        "        logger.store(LossQ=outs[0], QVals=outs[1])\n",
        "          \n",
        "        # Policy update\n",
        "        outs = sess.run([pi_loss, train_pi_opt, target_update], feed_dict)\n",
        "        logger.store(LossPi=outs[0])\n",
        "          \n",
        "      logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
        "      o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "        \n",
        "    # End of epoch wrap-up\n",
        "    if t > 0 and t % steps_per_epoch == 0:\n",
        "      epoch = t // steps_per_epoch\n",
        "      \n",
        "      # Save model\n",
        "      if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
        "        logger.save_state({'env': env}, None)\n",
        "        # Log info about epoch\n",
        "        logger.log_tabular('Epoch', epoch)\n",
        "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
        "        #logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
        "        logger.log_tabular('EpLen', average_only=True)\n",
        "        #logger.log_tabular('TestEpLen', average_only=True)\n",
        "        logger.log_tabular('TotalEnvInteracts', t)\n",
        "        logger.log_tabular('QVals', with_min_and_max=True)\n",
        "        logger.log_tabular('LossPi', average_only=True)\n",
        "        logger.log_tabular('LossQ', average_only=True)\n",
        "        logger.log_tabular('Time', time.time()-start_time)\n",
        "        logger.dump_tabular()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHq6wG5E8PhM"
      },
      "source": [
        "### Run DDPG with CartPole Env."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9RlXGSuU3f_"
      },
      "source": [
        "# load RL packages\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "\n",
        "# after training, load policy and show results in video\n",
        "#from spinup.utils.test_policy import load_policy, run_policy\n",
        "\n",
        "from spinup.utils.test_policy import load_policy_and_env, run_policy\n",
        "\n",
        "\n",
        "# global parameters\n",
        "env_name = 'LunarLanderContinuous-v2'\n",
        "output_dir = '/content/DDPG/LunarLanderContinuous-v2-1'\n",
        "\n",
        "# prepare ddpg parameters\n",
        "env_fn = lambda : gym.make(env_name)\n",
        "\n",
        "ac_kwargs = dict(hidden_sizes=[64, 64], activation=tf.nn.relu) # [128, 128] or [256, 256]\n",
        "\n",
        "logger_kwargs = dict(output_dir=output_dir, exp_name=env_name)\n",
        "\n",
        "##### Exp log\n",
        "# first attempt 'LunarLander-v2'\n",
        "# spinup ddpg does not work because the action space is discrete\n",
        "#env = gym.make('LunarLander-v2') \n",
        "#> action_space is discrete(2) \n",
        "#env.action_space.shape\n",
        "#print(env.action_space.shape[0])\n",
        "#act_dim = env.action_space.shape[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbP7I205TM7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f565120-bc17-42d6-e339-c80b75cc573e"
      },
      "source": [
        "ddpg(env_fn=env_fn, ac_kwargs=ac_kwargs, steps_per_epoch=5000, epochs=1, logger_kwargs=logger_kwargs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Log dir /content/DDPG/LunarLanderContinuous-v2-1 already exists! Storing info there anyway.\n",
            "\u001b[32;1mLogging data to /content/DDPG/LunarLanderContinuous-v2-1/progress.txt\u001b[0m\n",
            "\u001b[36;1mSaving config:\n",
            "\u001b[0m\n",
            "{\n",
            "    \"ac_kwargs\":\t{\n",
            "        \"activation\":\t\"relu\",\n",
            "        \"hidden_sizes\":\t[\n",
            "            64,\n",
            "            64\n",
            "        ]\n",
            "    },\n",
            "    \"act_noise\":\t0.1,\n",
            "    \"actor_critic\":\t\"mlp_actor_critic\",\n",
            "    \"batch_size\":\t100,\n",
            "    \"env_fn\":\t\"<function <lambda> at 0x7f0d202fc488>\",\n",
            "    \"epochs\":\t1,\n",
            "    \"exp_name\":\t\"LunarLanderContinuous-v2\",\n",
            "    \"gamma\":\t0.99,\n",
            "    \"logger\":\t{\n",
            "        \"<spinup.utils.logx.EpochLogger object at 0x7f0d9e0a9c88>\":\t{\n",
            "            \"epoch_dict\":\t{},\n",
            "            \"exp_name\":\t\"LunarLanderContinuous-v2\",\n",
            "            \"first_row\":\ttrue,\n",
            "            \"log_current_row\":\t{},\n",
            "            \"log_headers\":\t[],\n",
            "            \"output_dir\":\t\"/content/DDPG/LunarLanderContinuous-v2-1\",\n",
            "            \"output_file\":\t{\n",
            "                \"<_io.TextIOWrapper name='/content/DDPG/LunarLanderContinuous-v2-1/progress.txt' mode='w' encoding='UTF-8'>\":\t{\n",
            "                    \"mode\":\t\"w\"\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"logger_kwargs\":\t{\n",
            "        \"exp_name\":\t\"LunarLanderContinuous-v2\",\n",
            "        \"output_dir\":\t\"/content/DDPG/LunarLanderContinuous-v2-1\"\n",
            "    },\n",
            "    \"max_ep_len\":\t1000,\n",
            "    \"pi_lr\":\t0.001,\n",
            "    \"polyak\":\t0.995,\n",
            "    \"q_lr\":\t0.001,\n",
            "    \"replay_size\":\t1000000,\n",
            "    \"save_freq\":\t1,\n",
            "    \"seed\":\t0,\n",
            "    \"start_steps\":\t10000,\n",
            "    \"steps_per_epoch\":\t5000\n",
            "}\n",
            "ddpg obs_dim  8\n",
            "ddpg act_dim  2\n",
            "ddpg act_limit  1.0\n",
            "printing env.action_space  Box(2,)\n",
            "****** X_ph ****\n",
            "Tensor(\"Placeholder:0\", shape=(?, 8), dtype=float32)\n",
            "****** a_ph ****\n",
            "Tensor(\"Placeholder_1:0\", shape=(?, 2), dtype=float32)\n",
            "running \n",
            "before with, act_dim=  2\n",
            "act_limit  1.0\n",
            "calling pi in actor_critic\n",
            "hidden_size=  [64, 64]\n",
            "act_dim=  2\n",
            "calling q in actor_critic\n",
            "hidden_size=  [64, 64]\n",
            "calling q 2nd time in actor_critic\n",
            "hidden_size=  [64, 64]\n",
            "target\n",
            "before with, act_dim=  2\n",
            "act_limit  1.0\n",
            "calling pi in actor_critic\n",
            "hidden_size=  [64, 64]\n",
            "act_dim=  2\n",
            "calling q in actor_critic\n",
            "hidden_size=  [64, 64]\n",
            "calling q 2nd time in actor_critic\n",
            "hidden_size=  [64, 64]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}