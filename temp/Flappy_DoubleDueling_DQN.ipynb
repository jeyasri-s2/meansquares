{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": " Flappy_DoubleDueling_DQN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarsanjani/meansquares/blob/master/temp/Flappy_DoubleDueling_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0dL2PwhXNgF"
      },
      "source": [
        "## Double Dueling Deep Q Network Learning with Priortized Experienced Reply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W437Q5gRY2jQ"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1nxr3n4pfzZ",
        "outputId": "5ccfef63-f6fb-4558-9543-4022e452be85"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHpkDkDhYz2t",
        "outputId": "e929ec99-2100-4081-c0d4-c7de80c6d367"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install opencv-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.7.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV4RaAvlXNgF"
      },
      "source": [
        "import os,sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeiMuN03ZjYg"
      },
      "source": [
        "os.chdir('../')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfeCgBQ8ZnIz",
        "outputId": "75b505ea-411c-4051-c7ab-a334e2a5ef03"
      },
      "source": [
        "!git clone https://github.com/adityajn105/flappy-bird-deep-q-learning.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'flappy-bird-deep-q-learning'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Total 88 (delta 0), reused 0 (delta 0), pack-reused 88\u001b[K\n",
            "Unpacking objects: 100% (88/88), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhZubFpWZzRZ",
        "outputId": "4b1ded1f-9465-4b5f-f46d-51bca4f6f9ae"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin\t etc\t\t\t      lib64  root   sys\t\t       var\n",
            "boot\t flappy-bird-deep-q-learning  media  run    tensorflow-1.15.2\n",
            "content  home\t\t\t      mnt    sbin   tmp\n",
            "datalab  lib\t\t\t      opt    srv    tools\n",
            "dev\t lib32\t\t\t      proc   swift  usr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-VXDvm7Z781",
        "outputId": "237d374a-b7e6-4683-dd72-c2f6bbeb4e61"
      },
      "source": [
        "os.chdir('flappy-bird-deep-q-learning')\n",
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " assets\t\t\t  game\t\t README.md\n",
            " checkpoints\t\t  LICENSE\t requirements.txt\n",
            "'Deep Q Learning.ipynb'   play_game.py\t screenshots\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLiPvf-yZaZv"
      },
      "source": [
        "#sys.path.append('game/')\n",
        "os.chdir('game')\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeE_wCOOb2fe",
        "outputId": "941f5d39-68c8-49ee-c5ce-859eddb9e268"
      },
      "source": [
        "!pip install pygame "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/4c/2ebe8ab1a695a446574bc48d96eb3503649893be8c769e7fafd65fd18833/pygame-2.0.0-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5MB 5.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76AF_irCbUNc",
        "outputId": "fda992e3-e787-4cef-8c8d-c58feb0dd1a0"
      },
      "source": [
        "import pygame\n",
        "DISPLAY = True\n",
        "if not DISPLAY:\n",
        "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 2.0.0 (SDL 2.0.12, python 3.6.9)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPOjpx62q0yG"
      },
      "source": [
        "# Not working part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHLNn6zudU7F",
        "outputId": "2f7dd146-d2fb-4139-b00a-0cd79c0abfae"
      },
      "source": [
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))\n",
        "pygame.init()\n",
        "pygame.display.init()\n",
        "print( pygame.display.list_modes() )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(640, 480)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK2kfNV4dhui",
        "outputId": "ea76a0c1-6239-4462-f1ec-8457246d045a"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flappy_utils.py  flappy_wrapped.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8hRyunlnsqm",
        "outputId": "b02396cb-9608-46cc-bb30-c0e538e5ea9b"
      },
      "source": [
        "os.getcwd()\n",
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flappy_utils.py  flappy_wrapped.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-tF3Knjd6Ko"
      },
      "source": [
        "sys.path.append('game/')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "SUqTQyRboM1E",
        "outputId": "0fea6483-d4da-4e08-e2e0-ea7d910d7a05"
      },
      "source": [
        "import flappy_utils\n",
        "import flappy_wrapped as game"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c143cea0172a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflappy_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mflappy_wrapped\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/flappy-bird-deep-q-learning/game/flappy_wrapped.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Flappy Bird'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mIMAGES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOUNDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHITMASKS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflappy_bird_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mPIPEGAPSIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# gap between upper and lower part of pipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mBASEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSCREENHEIGHT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.79\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/flappy-bird-deep-q-learning/game/flappy_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# numbers sprites for score display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     IMAGES['numbers'] = (\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'assets/sprites/0.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'assets/sprites/1.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'assets/sprites/2.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12O-rWbimq5H"
      },
      "source": [
        "#Loading *flappy_wrapped* from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXGd7ar_nKFc",
        "outputId": "4bcbfab4-020b-4ab5-f4c4-ed8db91b11a5"
      },
      "source": [
        "import os \n",
        "\n",
        "# Set your working directory to a folder in your Google Drive. This way, if your notebook times out,\n",
        "# your files will be saved in your Google Drive!\n",
        "\n",
        "# the base Google Drive directory\n",
        "root_dir = \"/content/drive/Shared drives/\"\n",
        "\n",
        "# choose where you want your project files to be saved\n",
        "project_folder = \"MeanSquare-Drive/RL-assignment/flappy-bird-deep-q-learning/\"\n",
        "\n",
        "def create_and_set_working_directory(project_folder):\n",
        "  # check if your project folder exists. if not, it will be created.\n",
        "  if os.path.isdir(root_dir + project_folder) == False:\n",
        "    os.mkdir(root_dir + project_folder)\n",
        "    print(root_dir + project_folder + ' did not exist but was created.')\n",
        "\n",
        "  # change the OS to use your project folder as the working directory\n",
        "  os.chdir(root_dir + project_folder)\n",
        "\n",
        "  # create a test file to make sure it shows up in the right place\n",
        "  !touch 'new_file_in_working_directory.txt'\n",
        "  print('\\nYour working directory was changed to ' + root_dir + project_folder + \\\n",
        "        \"\\n\\nAn empty text file was created there. You can also run !pwd to confirm the current working directory.\" )\n",
        "\n",
        "create_and_set_working_directory(project_folder)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Your working directory was changed to /content/drive/Shared drives/MeanSquare-Drive/RL-assignment/flappy-bird-deep-q-learning/\n",
            "\n",
            "An empty text file was created there. You can also run !pwd to confirm the current working directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhLJX2ncncSI"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/Shared drives/MeanSquare-Drive/RL-assignment/flappy-bird-deep-q-learning')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Yd052Ebanhq7",
        "outputId": "ff0ee072-a88d-44cf-e220-be85b78b02f2"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/Shared drives/MeanSquare-Drive/RL-assignment/flappy-bird-deep-q-learning'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOYpBoRybqLT"
      },
      "source": [
        "import flappy_wrapped as game"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fHEFHKm0Wu"
      },
      "source": [
        "# KERNEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-PPnNO2XNgF"
      },
      "source": [
        "KERNEL = np.array([[-1,-1,-1], [-1, 9,-1],[-1,-1,-1]])\n",
        "def processFrame(frame):\n",
        "    frame = frame[55:288,0:400] #crop image\n",
        "    frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #convert image to black and white\n",
        "    frame = cv2.resize(frame,(84,84),interpolation=cv2.INTER_AREA)\n",
        "    _ , frame = cv2.threshold(frame,50,255,cv2.THRESH_BINARY)\n",
        "    #frame = cv2.blur(frame,(5,5))\n",
        "    frame = cv2.filter2D(frame,-1,KERNEL)\n",
        "    #frame = cv2.Canny(frame,100,200)\n",
        "    frame = frame.astype(np.float64)/255.0\n",
        "    return frame"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUp1niiUp0J5"
      },
      "source": [
        "# Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXjfXGmdXNgF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#Dueling DQN\n",
        "class DDQN(nn.Module):\n",
        "    def __init__(self,input_shape,nactions):\n",
        "        super(DDQN,self).__init__()\n",
        "        self.nactions = nactions\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0],32,kernel_size=4,stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,64,kernel_size=3,stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64,64,kernel_size=2,stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        \n",
        "        #scs - action network with 'nactions with q value' as output\n",
        "        self.fca = nn.Sequential(\n",
        "            nn.Linear( conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear( 512, nactions )\n",
        "        )\n",
        "        \n",
        "        #scs - value network with single value output\n",
        "        self.fcv = nn.Sequential(\n",
        "            nn.Linear(conv_out_size,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,1)\n",
        "        )\n",
        "        \n",
        "    def _get_conv_out(self,shape):\n",
        "        o = self.conv( torch.zeros(1,*shape) )\n",
        "        return int(np.prod(o.size()))\n",
        "    \n",
        "    def forward(self,x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        action_v = self.fca(conv_out)\n",
        "        value_v = self.fcv(conv_out).expand(x.size(0), self.nactions)\n",
        "        #scs aggregation layer:  Q(s,a) = V(s) + A(s,a)\n",
        "        return value_v + action_v - action_v.mean(1).unsqueeze(1).expand(x.size(0), self.nactions)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1INjeA_p4dP"
      },
      "source": [
        "# Hyperparam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBab3NBGXNgF"
      },
      "source": [
        "ACTIONS = [0,1]\n",
        "EXPERIENCE_BUFFER_SIZE = 2000 #-> update\n",
        "STATE_DIM = 4\n",
        "GAMMA = 0.95  #0.99 # -> changed update\n",
        "EPSILON_START = 1\n",
        "EPSILON_FINAL = 0.001\n",
        "EPSILON_DECAY_FRAMES = (10**4)/3 # -> update\n",
        "MEAN_GOAL_REWARD = 10 #10 # +1 for escaping death\n",
        "BATCH_SIZE = 32 #-> update\n",
        "MIN_EXP_BUFFER_SIZE = 500 #-> update\n",
        "SYNC_TARGET_FRAMES = 30 # tau ,go in times of 30 or 30/2 value- change this value for differeny sync time with target_net\n",
        "LEARNING_RATE = 1e-4 #-> update\n",
        "SKIP_FRAME = 2 #-> update\n",
        "INITIAL_SKIP = [0,1,0,1,0,1,0,0,0,1,0,1,0,1,0,1,0,0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,0,1]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je_1gzbhp7sn"
      },
      "source": [
        "# ExperienceBuffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu8kxJg9XNgF"
      },
      "source": [
        "import collections\n",
        "class ExperienceBuffer():\n",
        "    def __init__(self,capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "        self.priority = collections.deque(maxlen=capacity)\n",
        "    \n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "        self.priority.clear()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    def append(self,exp,p):\n",
        "        self.buffer.append(exp)\n",
        "        self.priority.append(p)\n",
        "        \n",
        "    def sample(self,batch_size):\n",
        "        probs = np.array(self.priority)/sum(np.array(self.priority))\n",
        "        indices = np.random.choice( range(len(self.buffer)), batch_size, p = probs)\n",
        "        states,actions,rewards,dones,next_states = zip(*[ self.buffer[idx] for idx in indices ])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\\\n",
        "    np.array(dones,dtype=np.uint8), np.array(next_states)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsBGXEKhp-b6"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdPkRwlIXNgF"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self,env,buffer,state_buffer_size = STATE_DIM):\n",
        "        self.env = env\n",
        "        self.exp_buffer = buffer\n",
        "        self.state = collections.deque(maxlen = STATE_DIM)\n",
        "        self.next_state= collections.deque(maxlen = STATE_DIM)\n",
        "        self._reset()\n",
        "        \n",
        "    def _reset(self):\n",
        "        self.total_rewards = 0\n",
        "        self.state.clear()\n",
        "        self.next_state.clear()\n",
        "        \n",
        "        for i in INITIAL_SKIP[:-7]:\n",
        "            frame,reward,done = self.env.frame_step(i)\n",
        "            self.total_rewards+=reward\n",
        "            if done:\n",
        "                self._reset()\n",
        "        frame = processFrame(frame)\n",
        "        self.state.append(frame)\n",
        "        self.next_state.append(frame)\n",
        "\n",
        "        for i in INITIAL_SKIP[-7:-5]:\n",
        "            frame,reward,done = self.env.frame_step(i)\n",
        "            self.total_rewards+=reward\n",
        "            if done:\n",
        "                self._reset()\n",
        "        frame = processFrame(frame)\n",
        "        self.state.append(frame)\n",
        "        self.next_state.append(frame)\n",
        "        \n",
        "        for i in INITIAL_SKIP[-5:-3]:\n",
        "            frame,reward,done = self.env.frame_step(i)\n",
        "            self.total_rewards+=reward\n",
        "            if done:\n",
        "                self._reset()\n",
        "        frame = processFrame(frame)\n",
        "        self.state.append(frame)\n",
        "        self.next_state.append(frame)\n",
        "        \n",
        "        for i in INITIAL_SKIP[-3:-1]:\n",
        "            frame,reward,done = self.env.frame_step(i)\n",
        "            self.total_rewards+=reward\n",
        "            if done:\n",
        "                self._reset()\n",
        "        frame = processFrame(frame)\n",
        "        self.state.append(frame)\n",
        "        self.next_state.append(frame)\n",
        "    \n",
        "    def step(self,net,tgt_net,epsilon=0.9,device='cpu'):\n",
        "        self.total_rewards = 0\n",
        "        if np.random.random() < epsilon:\n",
        "            action = np.random.choice(ACTIONS)\n",
        "        else:\n",
        "            #scs taking state and action\n",
        "            state_v = torch.tensor(np.array([self.state],copy=False),dtype=torch.float32).to(device)\n",
        "            action = int(torch.argmax(net(state_v)))\n",
        "       \n",
        "        frame,reward,done = self.env.frame_step(action)\n",
        "        self.total_rewards += reward\n",
        "        for _ in range(SKIP_FRAME):\n",
        "                frame,reward,done =  self.env.frame_step(action)\n",
        "                self.total_rewards += reward\n",
        "                if done:\n",
        "                    break\n",
        "                    \n",
        "        frame = processFrame(frame)\n",
        "        self.next_state.append(frame)\n",
        "        \n",
        "        if len(self.next_state)==STATE_DIM and len(self.state)==STATE_DIM:\n",
        "            #PER - Prioritized Experience Replay\n",
        "            o = net( torch.tensor( np.array([self.state]),dtype=torch.float32).to(device)).to('cpu').detach().numpy()[0][action]\n",
        "            e = float(torch.max(tgt_net( torch.tensor( np.array([self.next_state]),dtype=torch.float32).to(device))))\n",
        "            #scs priority\n",
        "            p = abs(o-e)+0.0001\n",
        "            #print(\"priority \", p)\n",
        "            #scs (exp = state, action, reward, state_next), priority\n",
        "            self.exp_buffer.append((self.state.copy(),action,int(self.total_rewards),done,self.next_state.copy()),p)\n",
        "        \n",
        "        self.state.append(frame)\n",
        "        \n",
        "        end_reward = int(self.total_rewards)\n",
        "        if done:\n",
        "            self._reset()\n",
        "        \n",
        "        return end_reward"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhWwPqBPqBnC"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaGIaZClXNgF"
      },
      "source": [
        "def calc_loss(batch,net,tgt_net,device='cpu'):\n",
        "    states,actions,rewards,dones,next_states = batch\n",
        "    \n",
        "    states_v = torch.tensor(states,dtype=torch.float32).to(device)\n",
        "    actions_v = torch.tensor(actions,dtype=torch.long).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    dones_v = torch.ByteTensor(dones).to(device)\n",
        "    next_states_v = torch.tensor(next_states,dtype=torch.float32).to(device)\n",
        "    \n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1) # Q(S,A) = V + A\n",
        "    next_state_action_values = tgt_net(next_states_v).max(1)[0] # max(Q(A))\n",
        "    next_state_action_values[dones_v] = 0.0\n",
        "    next_state_action_values = next_state_action_values.detach() \n",
        "    \n",
        "    expected_values = rewards_v +  next_state_action_values * GAMMA # rewards + max(Q(A)) * discount_factor\n",
        "    return nn.MSELoss()(state_action_values,expected_values)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAjMYmNcqEx7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HTUzIvMBXNgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4bcc9ace-3511-4089-ee99-5591c82db54a"
      },
      "source": [
        "all_losses = []\n",
        "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
        "\n",
        "#Double Dueling DQN\n",
        "net = DDQN( (STATE_DIM,84,84), len(ACTIONS) ).to(device)\n",
        "#print(\"net: \", net)\n",
        "tgt_net = DDQN( (STATE_DIM,84,84), len(ACTIONS) ).to(device)\n",
        "#print(\"tgt_net: \",tgt_net)\n",
        "\n",
        "env = game.GameState()\n",
        "#print(\"env: \",env)\n",
        "buffer = ExperienceBuffer(EXPERIENCE_BUFFER_SIZE)\n",
        "#print(\"buffer: \",buffer)\n",
        "agent = Agent(env,buffer)\n",
        "#print(\"agent: \",agent)\n",
        "epsilon = EPSILON_START\n",
        "#print(\"epsilon: \",epsilon)\n",
        "optimizer = optim.Adam(net.parameters(),lr=LEARNING_RATE)\n",
        "#print(\"optimizer: \",optimizer)\n",
        "\n",
        "total_rewards = []\n",
        "best_mean_reward = float('-inf')\n",
        "last_mean = float('-inf')\n",
        "game_id = 0\n",
        "while True:\n",
        "    epsilon = max( EPSILON_FINAL , EPSILON_START - game_id/EPSILON_DECAY_FRAMES )\n",
        "    #print(\"inside loop, epsilon\",epsilon)\n",
        "    reward = agent.step(net,tgt_net,epsilon,device=device)\n",
        "    #print(\"reward: \",reward)\n",
        "    if reward != 0:\n",
        "        game_id += 1\n",
        "        total_rewards.append(reward)\n",
        "        mean_reward = np.mean(total_rewards[-100:])\n",
        "        #print(\"mean_reward: \",mean_reward)\n",
        "\n",
        "        if game_id%5 == 0:  #scs why 5 here?\n",
        "            print(\"GAME : {} | EPSILON : {:.4f} | MEAN REWARD : {}\".format( game_id, epsilon, mean_reward ))\n",
        "            #print(\"##############################################################\")\n",
        "        if best_mean_reward < mean_reward:\n",
        "            best_mean_reward = mean_reward\n",
        "            #print(\"best_mean_reward \", best_mean_reward)\n",
        "            #print(\"last_mean: \", last_mean)\n",
        "            if best_mean_reward - last_mean >= 0.1:\n",
        "                torch.save(net.state_dict(),'checkpoints/flappy_best_model.dat')\n",
        "                print(\"REWARD {} -> {}. Model Saved\".format(last_mean,mean_reward))\n",
        "                last_mean = best_mean_reward\n",
        "\n",
        "        if game_id % SYNC_TARGET_FRAMES == 0:\n",
        "            tgt_net.load_state_dict(net.state_dict()) # step of dwelling, using net DDQN weights to load target DDQN\n",
        "            \n",
        "        if mean_reward >= MEAN_GOAL_REWARD:\n",
        "            print(\"Learned in {} Games.\".format(game_id))\n",
        "            break\n",
        "    \n",
        "    if len(buffer) < MIN_EXP_BUFFER_SIZE:\n",
        "        continue\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    #print(\"batch: \", batch)\n",
        "    loss_t = calc_loss(batch,net,tgt_net,device=device)\n",
        "    #print(\"loss_t: \", float(loss_t))\n",
        "    all_losses.append(float(loss_t))\n",
        "    loss_t.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REWARD -inf -> -1.0. Model Saved\n",
            "GAME : 5 | EPSILON : 0.9988 | MEAN REWARD : -1.0\n",
            "GAME : 10 | EPSILON : 0.9973 | MEAN REWARD : -0.8\n",
            "REWARD -1.0 -> -0.8. Model Saved\n",
            "GAME : 15 | EPSILON : 0.9958 | MEAN REWARD : -0.8666666666666667\n",
            "GAME : 20 | EPSILON : 0.9943 | MEAN REWARD : -0.9\n",
            "GAME : 25 | EPSILON : 0.9928 | MEAN REWARD : -0.92\n",
            "GAME : 30 | EPSILON : 0.9913 | MEAN REWARD : -0.9333333333333333\n",
            "GAME : 35 | EPSILON : 0.9898 | MEAN REWARD : -0.9428571428571428\n",
            "GAME : 40 | EPSILON : 0.9883 | MEAN REWARD : -0.95\n",
            "GAME : 45 | EPSILON : 0.9868 | MEAN REWARD : -0.9111111111111111\n",
            "GAME : 50 | EPSILON : 0.9853 | MEAN REWARD : -0.92\n",
            "GAME : 55 | EPSILON : 0.9838 | MEAN REWARD : -0.9272727272727272\n",
            "GAME : 60 | EPSILON : 0.9823 | MEAN REWARD : -0.9333333333333333\n",
            "GAME : 65 | EPSILON : 0.9808 | MEAN REWARD : -0.9076923076923077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GAME : 70 | EPSILON : 0.9793 | MEAN REWARD : -0.9142857142857143\n",
            "GAME : 75 | EPSILON : 0.9778 | MEAN REWARD : -0.92\n",
            "GAME : 80 | EPSILON : 0.9763 | MEAN REWARD : -0.925\n",
            "GAME : 85 | EPSILON : 0.9748 | MEAN REWARD : -0.9058823529411765\n",
            "GAME : 90 | EPSILON : 0.9733 | MEAN REWARD : -0.8888888888888888\n",
            "GAME : 95 | EPSILON : 0.9718 | MEAN REWARD : -0.8947368421052632\n",
            "GAME : 100 | EPSILON : 0.9703 | MEAN REWARD : -0.9\n",
            "GAME : 105 | EPSILON : 0.9688 | MEAN REWARD : -0.9\n",
            "GAME : 110 | EPSILON : 0.9673 | MEAN REWARD : -0.92\n",
            "GAME : 115 | EPSILON : 0.9658 | MEAN REWARD : -0.92\n",
            "GAME : 120 | EPSILON : 0.9643 | MEAN REWARD : -0.88\n",
            "GAME : 125 | EPSILON : 0.9628 | MEAN REWARD : -0.88\n",
            "GAME : 130 | EPSILON : 0.9613 | MEAN REWARD : -0.88\n",
            "GAME : 135 | EPSILON : 0.9598 | MEAN REWARD : -0.86\n",
            "GAME : 140 | EPSILON : 0.9583 | MEAN REWARD : -0.84\n",
            "GAME : 145 | EPSILON : 0.9568 | MEAN REWARD : -0.84\n",
            "GAME : 150 | EPSILON : 0.9553 | MEAN REWARD : -0.82\n",
            "GAME : 155 | EPSILON : 0.9538 | MEAN REWARD : -0.8\n",
            "GAME : 160 | EPSILON : 0.9523 | MEAN REWARD : -0.8\n",
            "GAME : 165 | EPSILON : 0.9508 | MEAN REWARD : -0.82\n",
            "GAME : 170 | EPSILON : 0.9493 | MEAN REWARD : -0.82\n",
            "GAME : 175 | EPSILON : 0.9478 | MEAN REWARD : -0.8\n",
            "GAME : 180 | EPSILON : 0.9463 | MEAN REWARD : -0.8\n",
            "GAME : 185 | EPSILON : 0.9448 | MEAN REWARD : -0.82\n",
            "GAME : 190 | EPSILON : 0.9433 | MEAN REWARD : -0.84\n",
            "GAME : 195 | EPSILON : 0.9418 | MEAN REWARD : -0.84\n",
            "GAME : 200 | EPSILON : 0.9403 | MEAN REWARD : -0.84\n",
            "GAME : 205 | EPSILON : 0.9388 | MEAN REWARD : -0.84\n",
            "GAME : 210 | EPSILON : 0.9373 | MEAN REWARD : -0.84\n",
            "GAME : 215 | EPSILON : 0.9358 | MEAN REWARD : -0.84\n",
            "GAME : 220 | EPSILON : 0.9343 | MEAN REWARD : -0.86\n",
            "GAME : 225 | EPSILON : 0.9328 | MEAN REWARD : -0.86\n",
            "GAME : 230 | EPSILON : 0.9313 | MEAN REWARD : -0.86\n",
            "GAME : 235 | EPSILON : 0.9298 | MEAN REWARD : -0.88\n",
            "GAME : 240 | EPSILON : 0.9283 | MEAN REWARD : -0.88\n",
            "GAME : 245 | EPSILON : 0.9268 | MEAN REWARD : -0.9\n",
            "GAME : 250 | EPSILON : 0.9253 | MEAN REWARD : -0.9\n",
            "GAME : 255 | EPSILON : 0.9238 | MEAN REWARD : -0.9\n",
            "GAME : 260 | EPSILON : 0.9223 | MEAN REWARD : -0.9\n",
            "GAME : 265 | EPSILON : 0.9208 | MEAN REWARD : -0.88\n",
            "GAME : 270 | EPSILON : 0.9193 | MEAN REWARD : -0.88\n",
            "GAME : 275 | EPSILON : 0.9178 | MEAN REWARD : -0.9\n",
            "GAME : 280 | EPSILON : 0.9163 | MEAN REWARD : -0.9\n",
            "GAME : 285 | EPSILON : 0.9148 | MEAN REWARD : -0.88\n",
            "GAME : 290 | EPSILON : 0.9133 | MEAN REWARD : -0.88\n",
            "GAME : 295 | EPSILON : 0.9118 | MEAN REWARD : -0.88\n",
            "GAME : 300 | EPSILON : 0.9103 | MEAN REWARD : -0.86\n",
            "GAME : 305 | EPSILON : 0.9088 | MEAN REWARD : -0.84\n",
            "GAME : 310 | EPSILON : 0.9073 | MEAN REWARD : -0.84\n",
            "GAME : 315 | EPSILON : 0.9058 | MEAN REWARD : -0.82\n",
            "GAME : 320 | EPSILON : 0.9043 | MEAN REWARD : -0.84\n",
            "GAME : 325 | EPSILON : 0.9028 | MEAN REWARD : -0.84\n",
            "GAME : 330 | EPSILON : 0.9013 | MEAN REWARD : -0.82\n",
            "GAME : 335 | EPSILON : 0.8998 | MEAN REWARD : -0.82\n",
            "GAME : 340 | EPSILON : 0.8983 | MEAN REWARD : -0.84\n",
            "GAME : 345 | EPSILON : 0.8968 | MEAN REWARD : -0.82\n",
            "GAME : 350 | EPSILON : 0.8953 | MEAN REWARD : -0.82\n",
            "GAME : 355 | EPSILON : 0.8938 | MEAN REWARD : -0.84\n",
            "GAME : 360 | EPSILON : 0.8923 | MEAN REWARD : -0.82\n",
            "GAME : 365 | EPSILON : 0.8908 | MEAN REWARD : -0.84\n",
            "GAME : 370 | EPSILON : 0.8893 | MEAN REWARD : -0.84\n",
            "GAME : 375 | EPSILON : 0.8878 | MEAN REWARD : -0.84\n",
            "GAME : 380 | EPSILON : 0.8863 | MEAN REWARD : -0.84\n",
            "GAME : 385 | EPSILON : 0.8848 | MEAN REWARD : -0.86\n",
            "GAME : 390 | EPSILON : 0.8833 | MEAN REWARD : -0.86\n",
            "GAME : 395 | EPSILON : 0.8818 | MEAN REWARD : -0.86\n",
            "GAME : 400 | EPSILON : 0.8803 | MEAN REWARD : -0.86\n",
            "GAME : 405 | EPSILON : 0.8788 | MEAN REWARD : -0.86\n",
            "GAME : 410 | EPSILON : 0.8773 | MEAN REWARD : -0.84\n",
            "GAME : 415 | EPSILON : 0.8758 | MEAN REWARD : -0.86\n",
            "GAME : 420 | EPSILON : 0.8743 | MEAN REWARD : -0.86\n",
            "GAME : 425 | EPSILON : 0.8728 | MEAN REWARD : -0.86\n",
            "GAME : 430 | EPSILON : 0.8713 | MEAN REWARD : -0.84\n",
            "GAME : 435 | EPSILON : 0.8698 | MEAN REWARD : -0.84\n",
            "GAME : 440 | EPSILON : 0.8683 | MEAN REWARD : -0.84\n",
            "GAME : 445 | EPSILON : 0.8668 | MEAN REWARD : -0.86\n",
            "GAME : 450 | EPSILON : 0.8653 | MEAN REWARD : -0.88\n",
            "GAME : 455 | EPSILON : 0.8638 | MEAN REWARD : -0.86\n",
            "GAME : 460 | EPSILON : 0.8623 | MEAN REWARD : -0.88\n",
            "GAME : 465 | EPSILON : 0.8608 | MEAN REWARD : -0.88\n",
            "GAME : 470 | EPSILON : 0.8593 | MEAN REWARD : -0.88\n",
            "GAME : 475 | EPSILON : 0.8578 | MEAN REWARD : -0.88\n",
            "GAME : 480 | EPSILON : 0.8563 | MEAN REWARD : -0.86\n",
            "GAME : 485 | EPSILON : 0.8548 | MEAN REWARD : -0.86\n",
            "GAME : 490 | EPSILON : 0.8533 | MEAN REWARD : -0.86\n",
            "GAME : 495 | EPSILON : 0.8518 | MEAN REWARD : -0.86\n",
            "GAME : 500 | EPSILON : 0.8503 | MEAN REWARD : -0.88\n",
            "GAME : 505 | EPSILON : 0.8488 | MEAN REWARD : -0.88\n",
            "GAME : 510 | EPSILON : 0.8473 | MEAN REWARD : -0.9\n",
            "GAME : 515 | EPSILON : 0.8458 | MEAN REWARD : -0.88\n",
            "GAME : 520 | EPSILON : 0.8443 | MEAN REWARD : -0.88\n",
            "GAME : 525 | EPSILON : 0.8428 | MEAN REWARD : -0.86\n",
            "GAME : 530 | EPSILON : 0.8413 | MEAN REWARD : -0.88\n",
            "GAME : 535 | EPSILON : 0.8398 | MEAN REWARD : -0.88\n",
            "GAME : 540 | EPSILON : 0.8383 | MEAN REWARD : -0.86\n",
            "GAME : 545 | EPSILON : 0.8368 | MEAN REWARD : -0.84\n",
            "GAME : 550 | EPSILON : 0.8353 | MEAN REWARD : -0.84\n",
            "GAME : 555 | EPSILON : 0.8338 | MEAN REWARD : -0.84\n",
            "GAME : 560 | EPSILON : 0.8323 | MEAN REWARD : -0.84\n",
            "GAME : 565 | EPSILON : 0.8308 | MEAN REWARD : -0.84\n",
            "GAME : 570 | EPSILON : 0.8293 | MEAN REWARD : -0.84\n",
            "GAME : 575 | EPSILON : 0.8278 | MEAN REWARD : -0.84\n",
            "GAME : 580 | EPSILON : 0.8263 | MEAN REWARD : -0.84\n",
            "GAME : 585 | EPSILON : 0.8248 | MEAN REWARD : -0.82\n",
            "GAME : 590 | EPSILON : 0.8233 | MEAN REWARD : -0.82\n",
            "GAME : 595 | EPSILON : 0.8218 | MEAN REWARD : -0.82\n",
            "GAME : 600 | EPSILON : 0.8203 | MEAN REWARD : -0.82\n",
            "GAME : 605 | EPSILON : 0.8188 | MEAN REWARD : -0.84\n",
            "GAME : 610 | EPSILON : 0.8173 | MEAN REWARD : -0.82\n",
            "GAME : 615 | EPSILON : 0.8158 | MEAN REWARD : -0.84\n",
            "GAME : 620 | EPSILON : 0.8143 | MEAN REWARD : -0.82\n",
            "GAME : 625 | EPSILON : 0.8128 | MEAN REWARD : -0.84\n",
            "GAME : 630 | EPSILON : 0.8113 | MEAN REWARD : -0.84\n",
            "GAME : 635 | EPSILON : 0.8098 | MEAN REWARD : -0.78\n",
            "GAME : 640 | EPSILON : 0.8083 | MEAN REWARD : -0.8\n",
            "GAME : 645 | EPSILON : 0.8068 | MEAN REWARD : -0.8\n",
            "GAME : 650 | EPSILON : 0.8053 | MEAN REWARD : -0.8\n",
            "GAME : 655 | EPSILON : 0.8038 | MEAN REWARD : -0.82\n",
            "GAME : 660 | EPSILON : 0.8023 | MEAN REWARD : -0.82\n",
            "GAME : 665 | EPSILON : 0.8008 | MEAN REWARD : -0.82\n",
            "GAME : 670 | EPSILON : 0.7993 | MEAN REWARD : -0.8\n",
            "GAME : 675 | EPSILON : 0.7978 | MEAN REWARD : -0.76\n",
            "GAME : 680 | EPSILON : 0.7963 | MEAN REWARD : -0.78\n",
            "GAME : 685 | EPSILON : 0.7948 | MEAN REWARD : -0.8\n",
            "GAME : 690 | EPSILON : 0.7933 | MEAN REWARD : -0.8\n",
            "GAME : 695 | EPSILON : 0.7918 | MEAN REWARD : -0.8\n",
            "GAME : 700 | EPSILON : 0.7903 | MEAN REWARD : -0.8\n",
            "GAME : 705 | EPSILON : 0.7888 | MEAN REWARD : -0.8\n",
            "GAME : 710 | EPSILON : 0.7873 | MEAN REWARD : -0.82\n",
            "GAME : 715 | EPSILON : 0.7858 | MEAN REWARD : -0.8\n",
            "GAME : 720 | EPSILON : 0.7843 | MEAN REWARD : -0.82\n",
            "GAME : 725 | EPSILON : 0.7828 | MEAN REWARD : -0.82\n",
            "GAME : 730 | EPSILON : 0.7813 | MEAN REWARD : -0.82\n",
            "GAME : 735 | EPSILON : 0.7798 | MEAN REWARD : -0.88\n",
            "GAME : 740 | EPSILON : 0.7783 | MEAN REWARD : -0.88\n",
            "GAME : 745 | EPSILON : 0.7768 | MEAN REWARD : -0.88\n",
            "GAME : 750 | EPSILON : 0.7753 | MEAN REWARD : -0.88\n",
            "GAME : 755 | EPSILON : 0.7738 | MEAN REWARD : -0.88\n",
            "GAME : 760 | EPSILON : 0.7723 | MEAN REWARD : -0.86\n",
            "GAME : 765 | EPSILON : 0.7708 | MEAN REWARD : -0.84\n",
            "GAME : 770 | EPSILON : 0.7693 | MEAN REWARD : -0.82\n",
            "GAME : 775 | EPSILON : 0.7678 | MEAN REWARD : -0.86\n",
            "GAME : 780 | EPSILON : 0.7663 | MEAN REWARD : -0.86\n",
            "GAME : 785 | EPSILON : 0.7648 | MEAN REWARD : -0.84\n",
            "GAME : 790 | EPSILON : 0.7633 | MEAN REWARD : -0.82\n",
            "GAME : 795 | EPSILON : 0.7618 | MEAN REWARD : -0.78\n",
            "GAME : 800 | EPSILON : 0.7603 | MEAN REWARD : -0.74\n",
            "GAME : 805 | EPSILON : 0.7588 | MEAN REWARD : -0.72\n",
            "GAME : 810 | EPSILON : 0.7573 | MEAN REWARD : -0.72\n",
            "GAME : 815 | EPSILON : 0.7558 | MEAN REWARD : -0.74\n",
            "REWARD -0.8 -> -0.7. Model Saved\n",
            "GAME : 820 | EPSILON : 0.7543 | MEAN REWARD : -0.7\n",
            "GAME : 825 | EPSILON : 0.7528 | MEAN REWARD : -0.7\n",
            "GAME : 830 | EPSILON : 0.7513 | MEAN REWARD : -0.7\n",
            "GAME : 835 | EPSILON : 0.7498 | MEAN REWARD : -0.7\n",
            "GAME : 840 | EPSILON : 0.7483 | MEAN REWARD : -0.7\n",
            "GAME : 845 | EPSILON : 0.7468 | MEAN REWARD : -0.72\n",
            "GAME : 850 | EPSILON : 0.7453 | MEAN REWARD : -0.72\n",
            "GAME : 855 | EPSILON : 0.7438 | MEAN REWARD : -0.7\n",
            "GAME : 860 | EPSILON : 0.7423 | MEAN REWARD : -0.72\n",
            "GAME : 865 | EPSILON : 0.7408 | MEAN REWARD : -0.72\n",
            "GAME : 870 | EPSILON : 0.7393 | MEAN REWARD : -0.74\n",
            "GAME : 875 | EPSILON : 0.7378 | MEAN REWARD : -0.74\n",
            "GAME : 880 | EPSILON : 0.7363 | MEAN REWARD : -0.72\n",
            "GAME : 885 | EPSILON : 0.7348 | MEAN REWARD : -0.72\n",
            "GAME : 890 | EPSILON : 0.7333 | MEAN REWARD : -0.72\n",
            "GAME : 895 | EPSILON : 0.7318 | MEAN REWARD : -0.76\n",
            "GAME : 900 | EPSILON : 0.7303 | MEAN REWARD : -0.76\n",
            "GAME : 905 | EPSILON : 0.7288 | MEAN REWARD : -0.78\n",
            "GAME : 910 | EPSILON : 0.7273 | MEAN REWARD : -0.76\n",
            "GAME : 915 | EPSILON : 0.7258 | MEAN REWARD : -0.74\n",
            "GAME : 920 | EPSILON : 0.7243 | MEAN REWARD : -0.78\n",
            "GAME : 925 | EPSILON : 0.7228 | MEAN REWARD : -0.78\n",
            "GAME : 930 | EPSILON : 0.7213 | MEAN REWARD : -0.78\n",
            "GAME : 935 | EPSILON : 0.7198 | MEAN REWARD : -0.76\n",
            "GAME : 940 | EPSILON : 0.7183 | MEAN REWARD : -0.76\n",
            "GAME : 945 | EPSILON : 0.7168 | MEAN REWARD : -0.74\n",
            "GAME : 950 | EPSILON : 0.7153 | MEAN REWARD : -0.74\n",
            "GAME : 955 | EPSILON : 0.7138 | MEAN REWARD : -0.76\n",
            "GAME : 960 | EPSILON : 0.7123 | MEAN REWARD : -0.76\n",
            "GAME : 965 | EPSILON : 0.7108 | MEAN REWARD : -0.76\n",
            "GAME : 970 | EPSILON : 0.7093 | MEAN REWARD : -0.78\n",
            "GAME : 975 | EPSILON : 0.7078 | MEAN REWARD : -0.76\n",
            "GAME : 980 | EPSILON : 0.7063 | MEAN REWARD : -0.76\n",
            "GAME : 985 | EPSILON : 0.7048 | MEAN REWARD : -0.74\n",
            "GAME : 990 | EPSILON : 0.7033 | MEAN REWARD : -0.74\n",
            "GAME : 995 | EPSILON : 0.7018 | MEAN REWARD : -0.7\n",
            "GAME : 1000 | EPSILON : 0.7003 | MEAN REWARD : -0.74\n",
            "GAME : 1005 | EPSILON : 0.6988 | MEAN REWARD : -0.72\n",
            "GAME : 1010 | EPSILON : 0.6973 | MEAN REWARD : -0.72\n",
            "GAME : 1015 | EPSILON : 0.6958 | MEAN REWARD : -0.72\n",
            "GAME : 1020 | EPSILON : 0.6943 | MEAN REWARD : -0.7\n",
            "GAME : 1025 | EPSILON : 0.6928 | MEAN REWARD : -0.68\n",
            "GAME : 1030 | EPSILON : 0.6913 | MEAN REWARD : -0.68\n",
            "GAME : 1035 | EPSILON : 0.6898 | MEAN REWARD : -0.68\n",
            "GAME : 1040 | EPSILON : 0.6883 | MEAN REWARD : -0.66\n",
            "GAME : 1045 | EPSILON : 0.6868 | MEAN REWARD : -0.66\n",
            "GAME : 1050 | EPSILON : 0.6853 | MEAN REWARD : -0.62\n",
            "GAME : 1055 | EPSILON : 0.6838 | MEAN REWARD : -0.62\n",
            "GAME : 1060 | EPSILON : 0.6823 | MEAN REWARD : -0.62\n",
            "GAME : 1065 | EPSILON : 0.6808 | MEAN REWARD : -0.62\n",
            "GAME : 1070 | EPSILON : 0.6793 | MEAN REWARD : -0.6\n",
            "REWARD -0.7 -> -0.58. Model Saved\n",
            "GAME : 1075 | EPSILON : 0.6778 | MEAN REWARD : -0.6\n",
            "GAME : 1080 | EPSILON : 0.6763 | MEAN REWARD : -0.62\n",
            "GAME : 1085 | EPSILON : 0.6748 | MEAN REWARD : -0.66\n",
            "GAME : 1090 | EPSILON : 0.6733 | MEAN REWARD : -0.66\n",
            "GAME : 1095 | EPSILON : 0.6718 | MEAN REWARD : -0.64\n",
            "GAME : 1100 | EPSILON : 0.6703 | MEAN REWARD : -0.64\n",
            "GAME : 1105 | EPSILON : 0.6688 | MEAN REWARD : -0.64\n",
            "GAME : 1110 | EPSILON : 0.6673 | MEAN REWARD : -0.62\n",
            "GAME : 1115 | EPSILON : 0.6658 | MEAN REWARD : -0.62\n",
            "GAME : 1120 | EPSILON : 0.6643 | MEAN REWARD : -0.64\n",
            "GAME : 1125 | EPSILON : 0.6628 | MEAN REWARD : -0.64\n",
            "GAME : 1130 | EPSILON : 0.6613 | MEAN REWARD : -0.64\n",
            "GAME : 1135 | EPSILON : 0.6598 | MEAN REWARD : -0.6\n",
            "GAME : 1140 | EPSILON : 0.6583 | MEAN REWARD : -0.62\n",
            "GAME : 1145 | EPSILON : 0.6568 | MEAN REWARD : -0.64\n",
            "GAME : 1150 | EPSILON : 0.6553 | MEAN REWARD : -0.66\n",
            "GAME : 1155 | EPSILON : 0.6538 | MEAN REWARD : -0.66\n",
            "GAME : 1160 | EPSILON : 0.6523 | MEAN REWARD : -0.64\n",
            "GAME : 1165 | EPSILON : 0.6508 | MEAN REWARD : -0.66\n",
            "GAME : 1170 | EPSILON : 0.6493 | MEAN REWARD : -0.68\n",
            "GAME : 1175 | EPSILON : 0.6478 | MEAN REWARD : -0.66\n",
            "GAME : 1180 | EPSILON : 0.6463 | MEAN REWARD : -0.66\n",
            "GAME : 1185 | EPSILON : 0.6448 | MEAN REWARD : -0.64\n",
            "GAME : 1190 | EPSILON : 0.6433 | MEAN REWARD : -0.62\n",
            "GAME : 1195 | EPSILON : 0.6418 | MEAN REWARD : -0.66\n",
            "GAME : 1200 | EPSILON : 0.6403 | MEAN REWARD : -0.64\n",
            "GAME : 1205 | EPSILON : 0.6388 | MEAN REWARD : -0.66\n",
            "GAME : 1210 | EPSILON : 0.6373 | MEAN REWARD : -0.7\n",
            "GAME : 1215 | EPSILON : 0.6358 | MEAN REWARD : -0.72\n",
            "GAME : 1220 | EPSILON : 0.6343 | MEAN REWARD : -0.7\n",
            "GAME : 1225 | EPSILON : 0.6328 | MEAN REWARD : -0.7\n",
            "GAME : 1230 | EPSILON : 0.6313 | MEAN REWARD : -0.7\n",
            "GAME : 1235 | EPSILON : 0.6298 | MEAN REWARD : -0.72\n",
            "GAME : 1240 | EPSILON : 0.6283 | MEAN REWARD : -0.7\n",
            "GAME : 1245 | EPSILON : 0.6268 | MEAN REWARD : -0.68\n",
            "GAME : 1250 | EPSILON : 0.6253 | MEAN REWARD : -0.64\n",
            "GAME : 1255 | EPSILON : 0.6238 | MEAN REWARD : -0.62\n",
            "GAME : 1260 | EPSILON : 0.6223 | MEAN REWARD : -0.58\n",
            "GAME : 1265 | EPSILON : 0.6208 | MEAN REWARD : -0.58\n",
            "GAME : 1270 | EPSILON : 0.6193 | MEAN REWARD : -0.58\n",
            "GAME : 1275 | EPSILON : 0.6178 | MEAN REWARD : -0.58\n",
            "GAME : 1280 | EPSILON : 0.6163 | MEAN REWARD : -0.54\n",
            "GAME : 1285 | EPSILON : 0.6148 | MEAN REWARD : -0.56\n",
            "GAME : 1290 | EPSILON : 0.6133 | MEAN REWARD : -0.54\n",
            "GAME : 1295 | EPSILON : 0.6118 | MEAN REWARD : -0.54\n",
            "GAME : 1300 | EPSILON : 0.6103 | MEAN REWARD : -0.54\n",
            "GAME : 1305 | EPSILON : 0.6088 | MEAN REWARD : -0.52\n",
            "GAME : 1310 | EPSILON : 0.6073 | MEAN REWARD : -0.5\n",
            "GAME : 1315 | EPSILON : 0.6058 | MEAN REWARD : -0.48\n",
            "REWARD -0.58 -> -0.46. Model Saved\n",
            "GAME : 1320 | EPSILON : 0.6043 | MEAN REWARD : -0.48\n",
            "GAME : 1325 | EPSILON : 0.6028 | MEAN REWARD : -0.46\n",
            "GAME : 1330 | EPSILON : 0.6013 | MEAN REWARD : -0.46\n",
            "GAME : 1335 | EPSILON : 0.5998 | MEAN REWARD : -0.46\n",
            "GAME : 1340 | EPSILON : 0.5983 | MEAN REWARD : -0.44\n",
            "GAME : 1345 | EPSILON : 0.5968 | MEAN REWARD : -0.46\n",
            "GAME : 1350 | EPSILON : 0.5953 | MEAN REWARD : -0.52\n",
            "GAME : 1355 | EPSILON : 0.5938 | MEAN REWARD : -0.54\n",
            "GAME : 1360 | EPSILON : 0.5923 | MEAN REWARD : -0.56\n",
            "GAME : 1365 | EPSILON : 0.5908 | MEAN REWARD : -0.52\n",
            "GAME : 1370 | EPSILON : 0.5893 | MEAN REWARD : -0.5\n",
            "GAME : 1375 | EPSILON : 0.5878 | MEAN REWARD : -0.5\n",
            "GAME : 1380 | EPSILON : 0.5863 | MEAN REWARD : -0.52\n",
            "GAME : 1385 | EPSILON : 0.5848 | MEAN REWARD : -0.5\n",
            "GAME : 1390 | EPSILON : 0.5833 | MEAN REWARD : -0.56\n",
            "GAME : 1395 | EPSILON : 0.5818 | MEAN REWARD : -0.52\n",
            "GAME : 1400 | EPSILON : 0.5803 | MEAN REWARD : -0.48\n",
            "GAME : 1405 | EPSILON : 0.5788 | MEAN REWARD : -0.48\n",
            "GAME : 1410 | EPSILON : 0.5773 | MEAN REWARD : -0.48\n",
            "GAME : 1415 | EPSILON : 0.5758 | MEAN REWARD : -0.46\n",
            "GAME : 1420 | EPSILON : 0.5743 | MEAN REWARD : -0.46\n",
            "GAME : 1425 | EPSILON : 0.5728 | MEAN REWARD : -0.5\n",
            "GAME : 1430 | EPSILON : 0.5713 | MEAN REWARD : -0.5\n",
            "GAME : 1435 | EPSILON : 0.5698 | MEAN REWARD : -0.52\n",
            "GAME : 1440 | EPSILON : 0.5683 | MEAN REWARD : -0.54\n",
            "GAME : 1445 | EPSILON : 0.5668 | MEAN REWARD : -0.5\n",
            "GAME : 1450 | EPSILON : 0.5653 | MEAN REWARD : -0.48\n",
            "GAME : 1455 | EPSILON : 0.5638 | MEAN REWARD : -0.46\n",
            "GAME : 1460 | EPSILON : 0.5623 | MEAN REWARD : -0.48\n",
            "GAME : 1465 | EPSILON : 0.5608 | MEAN REWARD : -0.5\n",
            "GAME : 1470 | EPSILON : 0.5593 | MEAN REWARD : -0.5\n",
            "GAME : 1475 | EPSILON : 0.5578 | MEAN REWARD : -0.54\n",
            "GAME : 1480 | EPSILON : 0.5563 | MEAN REWARD : -0.52\n",
            "GAME : 1485 | EPSILON : 0.5548 | MEAN REWARD : -0.52\n",
            "GAME : 1490 | EPSILON : 0.5533 | MEAN REWARD : -0.5\n",
            "GAME : 1495 | EPSILON : 0.5518 | MEAN REWARD : -0.52\n",
            "GAME : 1500 | EPSILON : 0.5503 | MEAN REWARD : -0.54\n",
            "GAME : 1505 | EPSILON : 0.5488 | MEAN REWARD : -0.56\n",
            "GAME : 1510 | EPSILON : 0.5473 | MEAN REWARD : -0.54\n",
            "GAME : 1515 | EPSILON : 0.5458 | MEAN REWARD : -0.56\n",
            "GAME : 1520 | EPSILON : 0.5443 | MEAN REWARD : -0.5\n",
            "GAME : 1525 | EPSILON : 0.5428 | MEAN REWARD : -0.46\n",
            "GAME : 1530 | EPSILON : 0.5413 | MEAN REWARD : -0.48\n",
            "GAME : 1535 | EPSILON : 0.5398 | MEAN REWARD : -0.46\n",
            "GAME : 1540 | EPSILON : 0.5383 | MEAN REWARD : -0.48\n",
            "GAME : 1545 | EPSILON : 0.5368 | MEAN REWARD : -0.48\n",
            "GAME : 1550 | EPSILON : 0.5353 | MEAN REWARD : -0.48\n",
            "GAME : 1555 | EPSILON : 0.5338 | MEAN REWARD : -0.48\n",
            "GAME : 1560 | EPSILON : 0.5323 | MEAN REWARD : -0.46\n",
            "GAME : 1565 | EPSILON : 0.5308 | MEAN REWARD : -0.44\n",
            "GAME : 1570 | EPSILON : 0.5293 | MEAN REWARD : -0.46\n",
            "GAME : 1575 | EPSILON : 0.5278 | MEAN REWARD : -0.44\n",
            "GAME : 1580 | EPSILON : 0.5263 | MEAN REWARD : -0.44\n",
            "GAME : 1585 | EPSILON : 0.5248 | MEAN REWARD : -0.46\n",
            "GAME : 1590 | EPSILON : 0.5233 | MEAN REWARD : -0.46\n",
            "GAME : 1595 | EPSILON : 0.5218 | MEAN REWARD : -0.48\n",
            "GAME : 1600 | EPSILON : 0.5203 | MEAN REWARD : -0.48\n",
            "GAME : 1605 | EPSILON : 0.5188 | MEAN REWARD : -0.48\n",
            "GAME : 1610 | EPSILON : 0.5173 | MEAN REWARD : -0.5\n",
            "GAME : 1615 | EPSILON : 0.5158 | MEAN REWARD : -0.48\n",
            "GAME : 1620 | EPSILON : 0.5143 | MEAN REWARD : -0.54\n",
            "GAME : 1625 | EPSILON : 0.5128 | MEAN REWARD : -0.56\n",
            "GAME : 1630 | EPSILON : 0.5113 | MEAN REWARD : -0.54\n",
            "GAME : 1635 | EPSILON : 0.5098 | MEAN REWARD : -0.56\n",
            "GAME : 1640 | EPSILON : 0.5083 | MEAN REWARD : -0.52\n",
            "GAME : 1645 | EPSILON : 0.5068 | MEAN REWARD : -0.52\n",
            "GAME : 1650 | EPSILON : 0.5053 | MEAN REWARD : -0.54\n",
            "GAME : 1655 | EPSILON : 0.5038 | MEAN REWARD : -0.52\n",
            "GAME : 1660 | EPSILON : 0.5023 | MEAN REWARD : -0.54\n",
            "GAME : 1665 | EPSILON : 0.5008 | MEAN REWARD : -0.56\n",
            "GAME : 1670 | EPSILON : 0.4993 | MEAN REWARD : -0.54\n",
            "GAME : 1675 | EPSILON : 0.4978 | MEAN REWARD : -0.52\n",
            "GAME : 1680 | EPSILON : 0.4963 | MEAN REWARD : -0.54\n",
            "GAME : 1685 | EPSILON : 0.4948 | MEAN REWARD : -0.5\n",
            "GAME : 1690 | EPSILON : 0.4933 | MEAN REWARD : -0.48\n",
            "GAME : 1695 | EPSILON : 0.4918 | MEAN REWARD : -0.44\n",
            "GAME : 1700 | EPSILON : 0.4903 | MEAN REWARD : -0.46\n",
            "GAME : 1705 | EPSILON : 0.4888 | MEAN REWARD : -0.4\n",
            "GAME : 1710 | EPSILON : 0.4873 | MEAN REWARD : -0.4\n",
            "GAME : 1715 | EPSILON : 0.4858 | MEAN REWARD : -0.42\n",
            "GAME : 1720 | EPSILON : 0.4843 | MEAN REWARD : -0.42\n",
            "GAME : 1725 | EPSILON : 0.4828 | MEAN REWARD : -0.4\n",
            "GAME : 1730 | EPSILON : 0.4813 | MEAN REWARD : -0.38\n",
            "REWARD -0.46 -> -0.36. Model Saved\n",
            "GAME : 1735 | EPSILON : 0.4798 | MEAN REWARD : -0.36\n",
            "GAME : 1740 | EPSILON : 0.4783 | MEAN REWARD : -0.38\n",
            "GAME : 1745 | EPSILON : 0.4768 | MEAN REWARD : -0.38\n",
            "GAME : 1750 | EPSILON : 0.4753 | MEAN REWARD : -0.34\n",
            "GAME : 1755 | EPSILON : 0.4738 | MEAN REWARD : -0.34\n",
            "GAME : 1760 | EPSILON : 0.4723 | MEAN REWARD : -0.36\n",
            "GAME : 1765 | EPSILON : 0.4708 | MEAN REWARD : -0.32\n",
            "GAME : 1770 | EPSILON : 0.4693 | MEAN REWARD : -0.32\n",
            "GAME : 1775 | EPSILON : 0.4678 | MEAN REWARD : -0.34\n",
            "GAME : 1780 | EPSILON : 0.4663 | MEAN REWARD : -0.28\n",
            "GAME : 1785 | EPSILON : 0.4648 | MEAN REWARD : -0.26\n",
            "GAME : 1790 | EPSILON : 0.4633 | MEAN REWARD : -0.28\n",
            "GAME : 1795 | EPSILON : 0.4618 | MEAN REWARD : -0.28\n",
            "GAME : 1800 | EPSILON : 0.4603 | MEAN REWARD : -0.24\n",
            "REWARD -0.36 -> -0.24. Model Saved\n",
            "GAME : 1805 | EPSILON : 0.4588 | MEAN REWARD : -0.28\n",
            "GAME : 1810 | EPSILON : 0.4573 | MEAN REWARD : -0.28\n",
            "GAME : 1815 | EPSILON : 0.4558 | MEAN REWARD : -0.3\n",
            "GAME : 1820 | EPSILON : 0.4543 | MEAN REWARD : -0.26\n",
            "GAME : 1825 | EPSILON : 0.4528 | MEAN REWARD : -0.28\n",
            "GAME : 1830 | EPSILON : 0.4513 | MEAN REWARD : -0.28\n",
            "GAME : 1835 | EPSILON : 0.4498 | MEAN REWARD : -0.28\n",
            "GAME : 1840 | EPSILON : 0.4483 | MEAN REWARD : -0.2\n",
            "GAME : 1845 | EPSILON : 0.4468 | MEAN REWARD : -0.2\n",
            "GAME : 1850 | EPSILON : 0.4453 | MEAN REWARD : -0.22\n",
            "GAME : 1855 | EPSILON : 0.4438 | MEAN REWARD : -0.26\n",
            "GAME : 1860 | EPSILON : 0.4423 | MEAN REWARD : -0.24\n",
            "GAME : 1865 | EPSILON : 0.4408 | MEAN REWARD : -0.26\n",
            "GAME : 1870 | EPSILON : 0.4393 | MEAN REWARD : -0.24\n",
            "GAME : 1875 | EPSILON : 0.4378 | MEAN REWARD : -0.22\n",
            "GAME : 1880 | EPSILON : 0.4363 | MEAN REWARD : -0.28\n",
            "GAME : 1885 | EPSILON : 0.4348 | MEAN REWARD : -0.3\n",
            "GAME : 1890 | EPSILON : 0.4333 | MEAN REWARD : -0.26\n",
            "GAME : 1895 | EPSILON : 0.4318 | MEAN REWARD : -0.26\n",
            "GAME : 1900 | EPSILON : 0.4303 | MEAN REWARD : -0.3\n",
            "GAME : 1905 | EPSILON : 0.4288 | MEAN REWARD : -0.32\n",
            "GAME : 1910 | EPSILON : 0.4273 | MEAN REWARD : -0.3\n",
            "GAME : 1915 | EPSILON : 0.4258 | MEAN REWARD : -0.24\n",
            "GAME : 1920 | EPSILON : 0.4243 | MEAN REWARD : -0.22\n",
            "GAME : 1925 | EPSILON : 0.4228 | MEAN REWARD : -0.16\n",
            "GAME : 1930 | EPSILON : 0.4213 | MEAN REWARD : -0.16\n",
            "GAME : 1935 | EPSILON : 0.4198 | MEAN REWARD : -0.16\n",
            "GAME : 1940 | EPSILON : 0.4183 | MEAN REWARD : -0.2\n",
            "GAME : 1945 | EPSILON : 0.4168 | MEAN REWARD : -0.16\n",
            "GAME : 1950 | EPSILON : 0.4153 | MEAN REWARD : -0.14\n",
            "REWARD -0.24 -> -0.12. Model Saved\n",
            "GAME : 1955 | EPSILON : 0.4138 | MEAN REWARD : -0.12\n",
            "GAME : 1960 | EPSILON : 0.4123 | MEAN REWARD : -0.08\n",
            "GAME : 1965 | EPSILON : 0.4108 | MEAN REWARD : -0.04\n",
            "GAME : 1970 | EPSILON : 0.4093 | MEAN REWARD : -0.06\n",
            "GAME : 1975 | EPSILON : 0.4078 | MEAN REWARD : -0.06\n",
            "GAME : 1980 | EPSILON : 0.4063 | MEAN REWARD : -0.06\n",
            "GAME : 1985 | EPSILON : 0.4048 | MEAN REWARD : -0.06\n",
            "GAME : 1990 | EPSILON : 0.4033 | MEAN REWARD : -0.06\n",
            "GAME : 1995 | EPSILON : 0.4018 | MEAN REWARD : -0.08\n",
            "GAME : 2000 | EPSILON : 0.4003 | MEAN REWARD : -0.06\n",
            "GAME : 2005 | EPSILON : 0.3988 | MEAN REWARD : -0.02\n",
            "REWARD -0.12 -> 0.0. Model Saved\n",
            "GAME : 2010 | EPSILON : 0.3973 | MEAN REWARD : -0.02\n",
            "GAME : 2015 | EPSILON : 0.3958 | MEAN REWARD : 0.0\n",
            "GAME : 2020 | EPSILON : 0.3943 | MEAN REWARD : -0.02\n",
            "GAME : 2025 | EPSILON : 0.3928 | MEAN REWARD : -0.1\n",
            "GAME : 2030 | EPSILON : 0.3913 | MEAN REWARD : -0.12\n",
            "GAME : 2035 | EPSILON : 0.3898 | MEAN REWARD : -0.12\n",
            "GAME : 2040 | EPSILON : 0.3883 | MEAN REWARD : -0.12\n",
            "GAME : 2045 | EPSILON : 0.3868 | MEAN REWARD : -0.16\n",
            "GAME : 2050 | EPSILON : 0.3853 | MEAN REWARD : -0.16\n",
            "GAME : 2055 | EPSILON : 0.3838 | MEAN REWARD : -0.14\n",
            "GAME : 2060 | EPSILON : 0.3823 | MEAN REWARD : -0.18\n",
            "GAME : 2065 | EPSILON : 0.3808 | MEAN REWARD : -0.22\n",
            "GAME : 2070 | EPSILON : 0.3793 | MEAN REWARD : -0.24\n",
            "GAME : 2075 | EPSILON : 0.3778 | MEAN REWARD : -0.2\n",
            "GAME : 2080 | EPSILON : 0.3763 | MEAN REWARD : -0.16\n",
            "GAME : 2085 | EPSILON : 0.3748 | MEAN REWARD : -0.16\n",
            "GAME : 2090 | EPSILON : 0.3733 | MEAN REWARD : -0.16\n",
            "GAME : 2095 | EPSILON : 0.3718 | MEAN REWARD : -0.16\n",
            "GAME : 2100 | EPSILON : 0.3703 | MEAN REWARD : -0.14\n",
            "GAME : 2105 | EPSILON : 0.3688 | MEAN REWARD : -0.14\n",
            "GAME : 2110 | EPSILON : 0.3673 | MEAN REWARD : -0.12\n",
            "GAME : 2115 | EPSILON : 0.3658 | MEAN REWARD : -0.16\n",
            "GAME : 2120 | EPSILON : 0.3643 | MEAN REWARD : -0.22\n",
            "GAME : 2125 | EPSILON : 0.3628 | MEAN REWARD : -0.18\n",
            "GAME : 2130 | EPSILON : 0.3613 | MEAN REWARD : -0.18\n",
            "GAME : 2135 | EPSILON : 0.3598 | MEAN REWARD : -0.18\n",
            "GAME : 2140 | EPSILON : 0.3583 | MEAN REWARD : -0.2\n",
            "GAME : 2145 | EPSILON : 0.3568 | MEAN REWARD : -0.18\n",
            "GAME : 2150 | EPSILON : 0.3553 | MEAN REWARD : -0.18\n",
            "GAME : 2155 | EPSILON : 0.3538 | MEAN REWARD : -0.16\n",
            "GAME : 2160 | EPSILON : 0.3523 | MEAN REWARD : -0.14\n",
            "GAME : 2165 | EPSILON : 0.3508 | MEAN REWARD : -0.14\n",
            "GAME : 2170 | EPSILON : 0.3493 | MEAN REWARD : -0.08\n",
            "GAME : 2175 | EPSILON : 0.3478 | MEAN REWARD : -0.14\n",
            "GAME : 2180 | EPSILON : 0.3463 | MEAN REWARD : -0.16\n",
            "GAME : 2185 | EPSILON : 0.3448 | MEAN REWARD : -0.16\n",
            "GAME : 2190 | EPSILON : 0.3433 | MEAN REWARD : -0.2\n",
            "GAME : 2195 | EPSILON : 0.3418 | MEAN REWARD : -0.22\n",
            "GAME : 2200 | EPSILON : 0.3403 | MEAN REWARD : -0.26\n",
            "GAME : 2205 | EPSILON : 0.3388 | MEAN REWARD : -0.26\n",
            "GAME : 2210 | EPSILON : 0.3373 | MEAN REWARD : -0.26\n",
            "GAME : 2215 | EPSILON : 0.3358 | MEAN REWARD : -0.28\n",
            "GAME : 2220 | EPSILON : 0.3343 | MEAN REWARD : -0.24\n",
            "GAME : 2225 | EPSILON : 0.3328 | MEAN REWARD : -0.2\n",
            "GAME : 2230 | EPSILON : 0.3313 | MEAN REWARD : -0.16\n",
            "GAME : 2235 | EPSILON : 0.3298 | MEAN REWARD : -0.14\n",
            "GAME : 2240 | EPSILON : 0.3283 | MEAN REWARD : -0.14\n",
            "GAME : 2245 | EPSILON : 0.3268 | MEAN REWARD : -0.14\n",
            "GAME : 2250 | EPSILON : 0.3253 | MEAN REWARD : -0.12\n",
            "GAME : 2255 | EPSILON : 0.3238 | MEAN REWARD : -0.14\n",
            "GAME : 2260 | EPSILON : 0.3223 | MEAN REWARD : -0.14\n",
            "GAME : 2265 | EPSILON : 0.3208 | MEAN REWARD : -0.12\n",
            "GAME : 2270 | EPSILON : 0.3193 | MEAN REWARD : -0.14\n",
            "GAME : 2275 | EPSILON : 0.3178 | MEAN REWARD : -0.08\n",
            "GAME : 2280 | EPSILON : 0.3163 | MEAN REWARD : -0.06\n",
            "GAME : 2285 | EPSILON : 0.3148 | MEAN REWARD : -0.08\n",
            "GAME : 2290 | EPSILON : 0.3133 | MEAN REWARD : -0.02\n",
            "GAME : 2295 | EPSILON : 0.3118 | MEAN REWARD : 0.02\n",
            "GAME : 2300 | EPSILON : 0.3103 | MEAN REWARD : 0.06\n",
            "GAME : 2305 | EPSILON : 0.3088 | MEAN REWARD : 0.06\n",
            "REWARD 0.0 -> 0.1. Model Saved\n",
            "GAME : 2310 | EPSILON : 0.3073 | MEAN REWARD : 0.1\n",
            "GAME : 2315 | EPSILON : 0.3058 | MEAN REWARD : 0.16\n",
            "GAME : 2320 | EPSILON : 0.3043 | MEAN REWARD : 0.16\n",
            "GAME : 2325 | EPSILON : 0.3028 | MEAN REWARD : 0.16\n",
            "GAME : 2330 | EPSILON : 0.3013 | MEAN REWARD : 0.14\n",
            "GAME : 2335 | EPSILON : 0.2998 | MEAN REWARD : 0.1\n",
            "GAME : 2340 | EPSILON : 0.2983 | MEAN REWARD : 0.12\n",
            "GAME : 2345 | EPSILON : 0.2968 | MEAN REWARD : 0.1\n",
            "GAME : 2350 | EPSILON : 0.2953 | MEAN REWARD : 0.06\n",
            "GAME : 2355 | EPSILON : 0.2938 | MEAN REWARD : 0.06\n",
            "GAME : 2360 | EPSILON : 0.2923 | MEAN REWARD : 0.08\n",
            "GAME : 2365 | EPSILON : 0.2908 | MEAN REWARD : 0.1\n",
            "GAME : 2370 | EPSILON : 0.2893 | MEAN REWARD : 0.1\n",
            "GAME : 2375 | EPSILON : 0.2878 | MEAN REWARD : 0.08\n",
            "GAME : 2380 | EPSILON : 0.2863 | MEAN REWARD : 0.08\n",
            "GAME : 2385 | EPSILON : 0.2848 | MEAN REWARD : 0.14\n",
            "GAME : 2390 | EPSILON : 0.2833 | MEAN REWARD : 0.08\n",
            "GAME : 2395 | EPSILON : 0.2818 | MEAN REWARD : 0.06\n",
            "GAME : 2400 | EPSILON : 0.2803 | MEAN REWARD : 0.04\n",
            "GAME : 2405 | EPSILON : 0.2788 | MEAN REWARD : 0.08\n",
            "GAME : 2410 | EPSILON : 0.2773 | MEAN REWARD : 0.04\n",
            "GAME : 2415 | EPSILON : 0.2758 | MEAN REWARD : -0.02\n",
            "GAME : 2420 | EPSILON : 0.2743 | MEAN REWARD : 0.0\n",
            "GAME : 2425 | EPSILON : 0.2728 | MEAN REWARD : -0.06\n",
            "GAME : 2430 | EPSILON : 0.2713 | MEAN REWARD : -0.04\n",
            "GAME : 2435 | EPSILON : 0.2698 | MEAN REWARD : 0.0\n",
            "GAME : 2440 | EPSILON : 0.2683 | MEAN REWARD : 0.02\n",
            "GAME : 2445 | EPSILON : 0.2668 | MEAN REWARD : 0.04\n",
            "GAME : 2450 | EPSILON : 0.2653 | MEAN REWARD : 0.08\n",
            "GAME : 2455 | EPSILON : 0.2638 | MEAN REWARD : 0.06\n",
            "GAME : 2460 | EPSILON : 0.2623 | MEAN REWARD : 0.04\n",
            "GAME : 2465 | EPSILON : 0.2608 | MEAN REWARD : -0.02\n",
            "GAME : 2470 | EPSILON : 0.2593 | MEAN REWARD : -0.02\n",
            "GAME : 2475 | EPSILON : 0.2578 | MEAN REWARD : -0.02\n",
            "GAME : 2480 | EPSILON : 0.2563 | MEAN REWARD : -0.02\n",
            "GAME : 2485 | EPSILON : 0.2548 | MEAN REWARD : -0.04\n",
            "GAME : 2490 | EPSILON : 0.2533 | MEAN REWARD : -0.02\n",
            "GAME : 2495 | EPSILON : 0.2518 | MEAN REWARD : -0.02\n",
            "GAME : 2500 | EPSILON : 0.2503 | MEAN REWARD : -0.04\n",
            "GAME : 2505 | EPSILON : 0.2488 | MEAN REWARD : -0.08\n",
            "GAME : 2510 | EPSILON : 0.2473 | MEAN REWARD : -0.08\n",
            "GAME : 2515 | EPSILON : 0.2458 | MEAN REWARD : -0.04\n",
            "GAME : 2520 | EPSILON : 0.2443 | MEAN REWARD : -0.04\n",
            "GAME : 2525 | EPSILON : 0.2428 | MEAN REWARD : 0.02\n",
            "GAME : 2530 | EPSILON : 0.2413 | MEAN REWARD : 0.06\n",
            "GAME : 2535 | EPSILON : 0.2398 | MEAN REWARD : 0.04\n",
            "GAME : 2540 | EPSILON : 0.2383 | MEAN REWARD : 0.04\n",
            "GAME : 2545 | EPSILON : 0.2368 | MEAN REWARD : 0.04\n",
            "GAME : 2550 | EPSILON : 0.2353 | MEAN REWARD : 0.04\n",
            "GAME : 2555 | EPSILON : 0.2338 | MEAN REWARD : 0.1\n",
            "GAME : 2560 | EPSILON : 0.2323 | MEAN REWARD : 0.12\n",
            "GAME : 2565 | EPSILON : 0.2308 | MEAN REWARD : 0.12\n",
            "GAME : 2570 | EPSILON : 0.2293 | MEAN REWARD : 0.14\n",
            "GAME : 2575 | EPSILON : 0.2278 | MEAN REWARD : 0.16\n",
            "GAME : 2580 | EPSILON : 0.2263 | MEAN REWARD : 0.12\n",
            "GAME : 2585 | EPSILON : 0.2248 | MEAN REWARD : 0.14\n",
            "GAME : 2590 | EPSILON : 0.2233 | MEAN REWARD : 0.1\n",
            "GAME : 2595 | EPSILON : 0.2218 | MEAN REWARD : 0.12\n",
            "GAME : 2600 | EPSILON : 0.2203 | MEAN REWARD : 0.16\n",
            "REWARD 0.1 -> 0.2. Model Saved\n",
            "GAME : 2605 | EPSILON : 0.2188 | MEAN REWARD : 0.2\n",
            "GAME : 2610 | EPSILON : 0.2173 | MEAN REWARD : 0.2\n",
            "GAME : 2615 | EPSILON : 0.2158 | MEAN REWARD : 0.18\n",
            "GAME : 2620 | EPSILON : 0.2143 | MEAN REWARD : 0.22\n",
            "GAME : 2625 | EPSILON : 0.2128 | MEAN REWARD : 0.22\n",
            "GAME : 2630 | EPSILON : 0.2113 | MEAN REWARD : 0.16\n",
            "GAME : 2635 | EPSILON : 0.2098 | MEAN REWARD : 0.2\n",
            "GAME : 2640 | EPSILON : 0.2083 | MEAN REWARD : 0.22\n",
            "GAME : 2645 | EPSILON : 0.2068 | MEAN REWARD : 0.22\n",
            "GAME : 2650 | EPSILON : 0.2053 | MEAN REWARD : 0.2\n",
            "GAME : 2655 | EPSILON : 0.2038 | MEAN REWARD : 0.14\n",
            "GAME : 2660 | EPSILON : 0.2023 | MEAN REWARD : 0.16\n",
            "GAME : 2665 | EPSILON : 0.2008 | MEAN REWARD : 0.18\n",
            "GAME : 2670 | EPSILON : 0.1993 | MEAN REWARD : 0.14\n",
            "GAME : 2675 | EPSILON : 0.1978 | MEAN REWARD : 0.16\n",
            "GAME : 2680 | EPSILON : 0.1963 | MEAN REWARD : 0.2\n",
            "GAME : 2685 | EPSILON : 0.1948 | MEAN REWARD : 0.18\n",
            "GAME : 2690 | EPSILON : 0.1933 | MEAN REWARD : 0.26\n",
            "GAME : 2695 | EPSILON : 0.1918 | MEAN REWARD : 0.26\n",
            "GAME : 2700 | EPSILON : 0.1903 | MEAN REWARD : 0.24\n",
            "GAME : 2705 | EPSILON : 0.1888 | MEAN REWARD : 0.22\n",
            "GAME : 2710 | EPSILON : 0.1873 | MEAN REWARD : 0.24\n",
            "GAME : 2715 | EPSILON : 0.1858 | MEAN REWARD : 0.28\n",
            "GAME : 2720 | EPSILON : 0.1843 | MEAN REWARD : 0.24\n",
            "GAME : 2725 | EPSILON : 0.1828 | MEAN REWARD : 0.2\n",
            "GAME : 2730 | EPSILON : 0.1813 | MEAN REWARD : 0.24\n",
            "GAME : 2735 | EPSILON : 0.1798 | MEAN REWARD : 0.24\n",
            "GAME : 2740 | EPSILON : 0.1783 | MEAN REWARD : 0.24\n",
            "GAME : 2745 | EPSILON : 0.1768 | MEAN REWARD : 0.26\n",
            "GAME : 2750 | EPSILON : 0.1753 | MEAN REWARD : 0.26\n",
            "GAME : 2755 | EPSILON : 0.1738 | MEAN REWARD : 0.32\n",
            "REWARD 0.2 -> 0.32. Model Saved\n",
            "GAME : 2760 | EPSILON : 0.1723 | MEAN REWARD : 0.3\n",
            "GAME : 2765 | EPSILON : 0.1708 | MEAN REWARD : 0.28\n",
            "GAME : 2770 | EPSILON : 0.1693 | MEAN REWARD : 0.34\n",
            "GAME : 2775 | EPSILON : 0.1678 | MEAN REWARD : 0.34\n",
            "GAME : 2780 | EPSILON : 0.1663 | MEAN REWARD : 0.32\n",
            "GAME : 2785 | EPSILON : 0.1648 | MEAN REWARD : 0.32\n",
            "GAME : 2790 | EPSILON : 0.1633 | MEAN REWARD : 0.26\n",
            "GAME : 2795 | EPSILON : 0.1618 | MEAN REWARD : 0.3\n",
            "GAME : 2800 | EPSILON : 0.1603 | MEAN REWARD : 0.34\n",
            "GAME : 2805 | EPSILON : 0.1588 | MEAN REWARD : 0.34\n",
            "GAME : 2810 | EPSILON : 0.1573 | MEAN REWARD : 0.32\n",
            "GAME : 2815 | EPSILON : 0.1558 | MEAN REWARD : 0.32\n",
            "GAME : 2820 | EPSILON : 0.1543 | MEAN REWARD : 0.34\n",
            "GAME : 2825 | EPSILON : 0.1528 | MEAN REWARD : 0.38\n",
            "GAME : 2830 | EPSILON : 0.1513 | MEAN REWARD : 0.36\n",
            "GAME : 2835 | EPSILON : 0.1498 | MEAN REWARD : 0.36\n",
            "GAME : 2840 | EPSILON : 0.1483 | MEAN REWARD : 0.34\n",
            "GAME : 2845 | EPSILON : 0.1468 | MEAN REWARD : 0.34\n",
            "GAME : 2850 | EPSILON : 0.1453 | MEAN REWARD : 0.38\n",
            "GAME : 2855 | EPSILON : 0.1438 | MEAN REWARD : 0.36\n",
            "GAME : 2860 | EPSILON : 0.1423 | MEAN REWARD : 0.38\n",
            "REWARD 0.32 -> 0.44. Model Saved\n",
            "GAME : 2865 | EPSILON : 0.1408 | MEAN REWARD : 0.44\n",
            "GAME : 2870 | EPSILON : 0.1393 | MEAN REWARD : 0.44\n",
            "GAME : 2875 | EPSILON : 0.1378 | MEAN REWARD : 0.4\n",
            "GAME : 2880 | EPSILON : 0.1363 | MEAN REWARD : 0.44\n",
            "GAME : 2885 | EPSILON : 0.1348 | MEAN REWARD : 0.44\n",
            "GAME : 2890 | EPSILON : 0.1333 | MEAN REWARD : 0.5\n",
            "GAME : 2895 | EPSILON : 0.1318 | MEAN REWARD : 0.48\n",
            "GAME : 2900 | EPSILON : 0.1303 | MEAN REWARD : 0.46\n",
            "GAME : 2905 | EPSILON : 0.1288 | MEAN REWARD : 0.48\n",
            "GAME : 2910 | EPSILON : 0.1273 | MEAN REWARD : 0.52\n",
            "GAME : 2915 | EPSILON : 0.1258 | MEAN REWARD : 0.5\n",
            "GAME : 2920 | EPSILON : 0.1243 | MEAN REWARD : 0.5\n",
            "GAME : 2925 | EPSILON : 0.1228 | MEAN REWARD : 0.52\n",
            "REWARD 0.44 -> 0.54. Model Saved\n",
            "GAME : 2930 | EPSILON : 0.1213 | MEAN REWARD : 0.52\n",
            "GAME : 2935 | EPSILON : 0.1198 | MEAN REWARD : 0.5\n",
            "GAME : 2940 | EPSILON : 0.1183 | MEAN REWARD : 0.52\n",
            "GAME : 2945 | EPSILON : 0.1168 | MEAN REWARD : 0.54\n",
            "GAME : 2950 | EPSILON : 0.1153 | MEAN REWARD : 0.54\n",
            "GAME : 2955 | EPSILON : 0.1138 | MEAN REWARD : 0.56\n",
            "GAME : 2960 | EPSILON : 0.1123 | MEAN REWARD : 0.56\n",
            "GAME : 2965 | EPSILON : 0.1108 | MEAN REWARD : 0.54\n",
            "GAME : 2970 | EPSILON : 0.1093 | MEAN REWARD : 0.52\n",
            "GAME : 2975 | EPSILON : 0.1078 | MEAN REWARD : 0.54\n",
            "GAME : 2980 | EPSILON : 0.1063 | MEAN REWARD : 0.56\n",
            "GAME : 2985 | EPSILON : 0.1048 | MEAN REWARD : 0.54\n",
            "GAME : 2990 | EPSILON : 0.1033 | MEAN REWARD : 0.56\n",
            "GAME : 2995 | EPSILON : 0.1018 | MEAN REWARD : 0.54\n",
            "GAME : 3000 | EPSILON : 0.1003 | MEAN REWARD : 0.54\n",
            "GAME : 3005 | EPSILON : 0.0988 | MEAN REWARD : 0.5\n",
            "GAME : 3010 | EPSILON : 0.0973 | MEAN REWARD : 0.48\n",
            "GAME : 3015 | EPSILON : 0.0958 | MEAN REWARD : 0.52\n",
            "GAME : 3020 | EPSILON : 0.0943 | MEAN REWARD : 0.52\n",
            "GAME : 3025 | EPSILON : 0.0928 | MEAN REWARD : 0.52\n",
            "GAME : 3030 | EPSILON : 0.0913 | MEAN REWARD : 0.56\n",
            "GAME : 3035 | EPSILON : 0.0898 | MEAN REWARD : 0.6\n",
            "GAME : 3040 | EPSILON : 0.0883 | MEAN REWARD : 0.58\n",
            "GAME : 3045 | EPSILON : 0.0868 | MEAN REWARD : 0.54\n",
            "GAME : 3050 | EPSILON : 0.0853 | MEAN REWARD : 0.56\n",
            "GAME : 3055 | EPSILON : 0.0838 | MEAN REWARD : 0.58\n",
            "GAME : 3060 | EPSILON : 0.0823 | MEAN REWARD : 0.6\n",
            "GAME : 3065 | EPSILON : 0.0808 | MEAN REWARD : 0.64\n",
            "REWARD 0.54 -> 0.66. Model Saved\n",
            "GAME : 3070 | EPSILON : 0.0793 | MEAN REWARD : 0.68\n",
            "GAME : 3075 | EPSILON : 0.0778 | MEAN REWARD : 0.7\n",
            "GAME : 3080 | EPSILON : 0.0763 | MEAN REWARD : 0.68\n",
            "GAME : 3085 | EPSILON : 0.0748 | MEAN REWARD : 0.74\n",
            "GAME : 3090 | EPSILON : 0.0733 | MEAN REWARD : 0.74\n",
            "REWARD 0.66 -> 0.78. Model Saved\n",
            "GAME : 3095 | EPSILON : 0.0718 | MEAN REWARD : 0.78\n",
            "GAME : 3100 | EPSILON : 0.0703 | MEAN REWARD : 0.82\n",
            "GAME : 3105 | EPSILON : 0.0688 | MEAN REWARD : 0.88\n",
            "REWARD 0.78 -> 0.9. Model Saved\n",
            "GAME : 3110 | EPSILON : 0.0673 | MEAN REWARD : 0.9\n",
            "GAME : 3115 | EPSILON : 0.0658 | MEAN REWARD : 0.9\n",
            "GAME : 3120 | EPSILON : 0.0643 | MEAN REWARD : 0.9\n",
            "GAME : 3125 | EPSILON : 0.0628 | MEAN REWARD : 0.9\n",
            "GAME : 3130 | EPSILON : 0.0613 | MEAN REWARD : 0.9\n",
            "GAME : 3135 | EPSILON : 0.0598 | MEAN REWARD : 0.9\n",
            "GAME : 3140 | EPSILON : 0.0583 | MEAN REWARD : 0.92\n",
            "GAME : 3145 | EPSILON : 0.0568 | MEAN REWARD : 0.94\n",
            "GAME : 3150 | EPSILON : 0.0553 | MEAN REWARD : 0.92\n",
            "GAME : 3155 | EPSILON : 0.0538 | MEAN REWARD : 0.9\n",
            "GAME : 3160 | EPSILON : 0.0523 | MEAN REWARD : 0.88\n",
            "GAME : 3165 | EPSILON : 0.0508 | MEAN REWARD : 0.86\n",
            "GAME : 3170 | EPSILON : 0.0493 | MEAN REWARD : 0.86\n",
            "GAME : 3175 | EPSILON : 0.0478 | MEAN REWARD : 0.86\n",
            "GAME : 3180 | EPSILON : 0.0463 | MEAN REWARD : 0.86\n",
            "GAME : 3185 | EPSILON : 0.0448 | MEAN REWARD : 0.84\n",
            "GAME : 3190 | EPSILON : 0.0433 | MEAN REWARD : 0.82\n",
            "GAME : 3195 | EPSILON : 0.0418 | MEAN REWARD : 0.82\n",
            "GAME : 3200 | EPSILON : 0.0403 | MEAN REWARD : 0.78\n",
            "GAME : 3205 | EPSILON : 0.0388 | MEAN REWARD : 0.78\n",
            "GAME : 3210 | EPSILON : 0.0373 | MEAN REWARD : 0.76\n",
            "GAME : 3215 | EPSILON : 0.0358 | MEAN REWARD : 0.76\n",
            "GAME : 3220 | EPSILON : 0.0343 | MEAN REWARD : 0.76\n",
            "GAME : 3225 | EPSILON : 0.0328 | MEAN REWARD : 0.76\n",
            "GAME : 3230 | EPSILON : 0.0313 | MEAN REWARD : 0.76\n",
            "GAME : 3235 | EPSILON : 0.0298 | MEAN REWARD : 0.74\n",
            "GAME : 3240 | EPSILON : 0.0283 | MEAN REWARD : 0.74\n",
            "GAME : 3245 | EPSILON : 0.0268 | MEAN REWARD : 0.74\n",
            "GAME : 3250 | EPSILON : 0.0253 | MEAN REWARD : 0.76\n",
            "GAME : 3255 | EPSILON : 0.0238 | MEAN REWARD : 0.78\n",
            "GAME : 3260 | EPSILON : 0.0223 | MEAN REWARD : 0.8\n",
            "GAME : 3265 | EPSILON : 0.0208 | MEAN REWARD : 0.8\n",
            "GAME : 3270 | EPSILON : 0.0193 | MEAN REWARD : 0.78\n",
            "GAME : 3275 | EPSILON : 0.0178 | MEAN REWARD : 0.78\n",
            "GAME : 3280 | EPSILON : 0.0163 | MEAN REWARD : 0.8\n",
            "GAME : 3285 | EPSILON : 0.0148 | MEAN REWARD : 0.82\n",
            "GAME : 3290 | EPSILON : 0.0133 | MEAN REWARD : 0.84\n",
            "GAME : 3295 | EPSILON : 0.0118 | MEAN REWARD : 0.84\n",
            "GAME : 3300 | EPSILON : 0.0103 | MEAN REWARD : 0.88\n",
            "GAME : 3305 | EPSILON : 0.0088 | MEAN REWARD : 0.88\n",
            "GAME : 3310 | EPSILON : 0.0073 | MEAN REWARD : 0.9\n",
            "GAME : 3315 | EPSILON : 0.0058 | MEAN REWARD : 0.9\n",
            "GAME : 3320 | EPSILON : 0.0043 | MEAN REWARD : 0.92\n",
            "GAME : 3325 | EPSILON : 0.0028 | MEAN REWARD : 0.92\n",
            "GAME : 3330 | EPSILON : 0.0013 | MEAN REWARD : 0.92\n",
            "GAME : 3335 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3340 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3345 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3350 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3355 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3360 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3365 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 3370 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3375 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3380 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3385 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3390 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3395 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3400 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3405 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3410 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3415 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3420 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3425 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3430 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3435 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3440 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3445 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3450 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3455 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3460 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3465 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3470 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3475 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3480 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3485 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3490 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3495 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3500 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3505 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3510 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3515 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3520 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3525 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3530 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3535 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3540 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3545 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3550 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3555 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3560 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3565 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3570 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3575 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3580 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3585 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3590 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3595 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3600 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3605 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3610 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3615 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3620 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3625 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3630 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3635 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3640 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3645 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3650 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3655 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3660 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3665 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3670 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3675 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3680 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3685 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3690 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3695 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3700 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3705 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3710 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3715 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3720 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3725 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3730 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3735 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3740 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3745 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3750 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3755 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3760 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3765 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3770 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 3775 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 3780 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 3785 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 3790 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3795 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3800 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3805 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3810 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3815 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3820 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3825 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3830 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3835 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3840 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3845 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3850 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3855 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3860 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3865 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 3870 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3875 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3880 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3885 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 3890 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 3895 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3900 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3905 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3910 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3915 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3920 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3925 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3930 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3935 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3940 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3945 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3950 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3955 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3960 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3965 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3970 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3975 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3980 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3985 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3990 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 3995 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4000 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4005 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4010 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4015 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4020 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4025 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4030 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4035 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4040 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4045 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4050 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4055 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4060 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4065 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4070 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4075 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4080 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4085 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4090 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4095 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4100 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4105 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4110 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4115 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4120 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4125 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4130 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4135 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4140 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4145 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4150 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4155 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4160 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4165 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4170 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4175 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4180 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4185 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4190 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4195 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4200 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4205 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4210 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4215 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4220 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4225 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4230 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4235 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4240 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4245 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4250 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4255 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4260 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4265 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4270 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4275 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4280 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4285 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4290 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4295 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4300 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4305 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4310 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4315 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4320 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4325 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4330 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4335 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4340 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4345 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4350 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4355 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4360 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4365 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4370 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4375 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4380 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4385 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4390 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4395 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4400 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4405 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4410 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4415 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4420 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4425 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4430 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4435 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4440 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4445 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4450 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4455 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4460 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4465 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4470 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4475 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4480 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4485 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4490 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4495 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4500 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4505 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4510 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4515 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4520 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4525 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4530 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4535 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4540 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4545 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4550 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4555 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4560 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4565 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4570 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4575 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4580 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4585 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4590 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4595 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4600 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4605 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4610 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4615 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4620 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4625 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4630 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4635 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4640 | EPSILON : 0.0010 | MEAN REWARD : 0.92\n",
            "GAME : 4645 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4650 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4655 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4660 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4665 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4670 | EPSILON : 0.0010 | MEAN REWARD : 0.94\n",
            "GAME : 4675 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4680 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4685 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4690 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4695 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4700 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4705 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4710 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4715 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4720 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4725 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4730 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4735 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4740 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4745 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4750 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4755 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4760 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4765 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4770 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4775 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4780 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4785 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4790 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4795 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4800 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4805 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4810 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4815 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4820 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4825 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4830 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4835 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4840 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4845 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4850 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4855 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4860 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4865 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 4870 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4875 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4880 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4885 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4890 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4895 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4900 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4905 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4910 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4915 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4920 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4925 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4930 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4935 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4940 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4945 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4950 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4955 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4960 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4965 | EPSILON : 0.0010 | MEAN REWARD : 0.96\n",
            "GAME : 4970 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4975 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4980 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4985 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4990 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 4995 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 5000 | EPSILON : 0.0010 | MEAN REWARD : 0.98\n",
            "GAME : 5005 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5010 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5015 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5020 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5025 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5030 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5035 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5040 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5045 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5050 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5055 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5060 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5065 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n",
            "GAME : 5070 | EPSILON : 0.0010 | MEAN REWARD : 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-f250e958cacf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mEPSILON_FINAL\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mEPSILON_START\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgame_id\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mEPSILON_DECAY_FRAMES\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#print(\"inside loop, epsilon\",epsilon)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m#print(\"reward: \",reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-d7f5262caad0>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, net, tgt_net, epsilon, device)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSKIP_FRAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/Shareddrives/MeanSquare-Drive/RL-assignment/flappy-bird-deep-q-learning/game/flappy_wrapped.py\u001b[0m in \u001b[0;36mframe_step\u001b[0;34m(self, input_action)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     (self.playerx, self.playery))\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_surface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mFPSCLOCK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pygame/surfarray.py\u001b[0m in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mnumpysf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnumpysf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpysurfarray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnumpysf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pygame/_numpysurfarray.py\u001b[0m in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0msurface_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opvcJvJ_tQgm"
      },
      "source": [
        "# Run Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHfNZn9TtaLt"
      },
      "source": [
        "import play_game"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi4PDIPYtquO"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EgLRUXutwfN"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkF7eVt8tSrm"
      },
      "source": [
        "!python 'play_game.py' --model 'checkpoints/flappy_best_model_game.dat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8TNbyQMgDMV"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lji3qolgFpY"
      },
      "source": [
        "* Why state dim is 4 ?\n",
        "https://towardsdatascience.com/use-reinforcement-learning-to-train-a-flappy-bird-never-to-die-35b9625aaecc "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhb9idgHi9vg"
      },
      "source": [
        "# Explanation on Implementation\n",
        "\n",
        "\n",
        "Implementation\n",
        "* Implementing fixed q-targets is pretty straightforward:\n",
        "\n",
        "* First, we create two networks (DQNetwork, TargetNetwork)\n",
        "* Then, we create a function that will take our DQNetwork parameters and copy them to our TargetNetwork\n",
        "* Finally, during the training, we calculate the TD target using our target network. We update the target network with the DQNetwork every tau step (tau is an hyper-parameter that we define)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnCidZ12gEqy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}